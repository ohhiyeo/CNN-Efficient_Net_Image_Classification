{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eff_se.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4k_pjJKcER7"
      },
      "source": [
        "#####################################################################################\n",
        "#                                                                                   #\n",
        "#       SE:                                                                         #\n",
        "#       Epoch   0 Time    217.1 lr = 0.002000 avg loss = 0.016681 accuracy =  9.52  #\n",
        "#       Epoch   1 Time    215.3 lr = 0.041600 avg loss = 0.012784 accuracy = 29.42  #\n",
        "#       Epoch   2 Time    215.1 lr = 0.081200 avg loss = 0.010624 accuracy = 36.74  #\n",
        "#       Epoch   3 Time    214.6 lr = 0.120800 avg loss = 0.009530 accuracy = 41.72  #\n",
        "#       Epoch   4 Time    215.4 lr = 0.160400 avg loss = 0.008793 accuracy = 43.56  #\n",
        "#       Epoch   5 Time    218.1 lr = 0.200000 avg loss = 0.008264 accuracy = 47.42  #\n",
        "#       Epoch   6 Time    219.1 lr = 0.199795 avg loss = 0.007689 accuracy = 49.68  #\n",
        "#       Epoch   7 Time    220.5 lr = 0.199180 avg loss = 0.007285 accuracy = 54.36  #\n",
        "#       Epoch   8 Time    221.6 lr = 0.198158 avg loss = 0.006996 accuracy = 54.22  #\n",
        "#       Epoch   9 Time    220.2 lr = 0.196733 avg loss = 0.006755 accuracy = 56.78  #\n",
        "#       Epoch  10 Time    217.0 lr = 0.194911 avg loss = 0.006551 accuracy = 56.16  #\n",
        "#       Epoch  11 Time    214.1 lr = 0.192699 avg loss = 0.006363 accuracy = 59.78  #\n",
        "#       Epoch  12 Time    214.8 lr = 0.190107 avg loss = 0.006212 accuracy = 60.14  #\n",
        "#       Epoch  13 Time    215.3 lr = 0.187145 avg loss = 0.006057 accuracy = 61.98  #\n",
        "#       Epoch  14 Time    216.2 lr = 0.183825 avg loss = 0.005940 accuracy = 60.86  #\n",
        "#       Epoch  15 Time    217.2 lr = 0.180161 avg loss = 0.005808 accuracy = 61.92  #\n",
        "#       Epoch  16 Time    216.2 lr = 0.176168 avg loss = 0.005708 accuracy = 63.24  #\n",
        "#       Epoch  17 Time    216.3 lr = 0.171863 avg loss = 0.005597 accuracy = 62.36  #\n",
        "#       Epoch  18 Time    218.0 lr = 0.167263 avg loss = 0.005533 accuracy = 64.68  #\n",
        "#       Epoch  19 Time    220.0 lr = 0.162387 avg loss = 0.005435 accuracy = 60.16  #\n",
        "#       Epoch  20 Time    221.7 lr = 0.157254 avg loss = 0.005361 accuracy = 65.94  #\n",
        "#       Epoch  21 Time    221.9 lr = 0.151887 avg loss = 0.005259 accuracy = 64.92  #\n",
        "#       Epoch  22 Time    220.5 lr = 0.146308 avg loss = 0.005167 accuracy = 64.82  #\n",
        "#       Epoch  23 Time    218.1 lr = 0.140538 avg loss = 0.005096 accuracy = 65.02  #\n",
        "#       Epoch  24 Time    217.6 lr = 0.134602 avg loss = 0.005020 accuracy = 64.40  #\n",
        "#       Epoch  25 Time    218.5 lr = 0.128524 avg loss = 0.004936 accuracy = 63.64  #\n",
        "#       Epoch  26 Time    219.0 lr = 0.122330 avg loss = 0.004860 accuracy = 66.20  #\n",
        "#       Epoch  27 Time    218.3 lr = 0.116044 avg loss = 0.004790 accuracy = 67.10  #\n",
        "#       Epoch  28 Time    220.0 lr = 0.109693 avg loss = 0.004681 accuracy = 68.80  #\n",
        "#       Epoch  29 Time    219.9 lr = 0.103302 avg loss = 0.004632 accuracy = 67.02  #\n",
        "#       Epoch  30 Time    218.2 lr = 0.096898 avg loss = 0.004518 accuracy = 68.78  #\n",
        "#       Epoch  31 Time    217.4 lr = 0.090507 avg loss = 0.004435 accuracy = 67.94  #\n",
        "#       Epoch  32 Time    217.8 lr = 0.084156 avg loss = 0.004361 accuracy = 69.32  #\n",
        "#       Epoch  33 Time    217.3 lr = 0.077870 avg loss = 0.004288 accuracy = 67.60  #\n",
        "#       Epoch  34 Time    216.7 lr = 0.071676 avg loss = 0.004167 accuracy = 69.86  #\n",
        "#       Epoch  35 Time    216.2 lr = 0.065598 avg loss = 0.004102 accuracy = 69.52  #\n",
        "#       Epoch  36 Time    217.2 lr = 0.059662 avg loss = 0.004010 accuracy = 69.00  #\n",
        "#       Epoch  37 Time    217.3 lr = 0.053892 avg loss = 0.003953 accuracy = 70.08  #\n",
        "#       Epoch  38 Time    216.9 lr = 0.048313 avg loss = 0.003844 accuracy = 70.52  #\n",
        "#       Epoch  39 Time    215.8 lr = 0.042946 avg loss = 0.003776 accuracy = 71.46  #\n",
        "#       Epoch  40 Time    216.4 lr = 0.037813 avg loss = 0.003676 accuracy = 71.46  #\n",
        "#       Epoch  41 Time    217.4 lr = 0.032937 avg loss = 0.003587 accuracy = 70.64  #\n",
        "#       Epoch  42 Time    217.1 lr = 0.028337 avg loss = 0.003519 accuracy = 70.52  #\n",
        "#       Epoch  43 Time    217.2 lr = 0.024032 avg loss = 0.003436 accuracy = 72.54  #\n",
        "#       Epoch  44 Time    217.2 lr = 0.020039 avg loss = 0.003326 accuracy = 71.66  #\n",
        "#       Epoch  45 Time    216.8 lr = 0.016375 avg loss = 0.003266 accuracy = 72.78  #   \n",
        "#       Epoch  46 Time    216.8 lr = 0.013055 avg loss = 0.003224 accuracy = 72.98  #\n",
        "#       Epoch  47 Time    219.8 lr = 0.010093 avg loss = 0.003118 accuracy = 73.34  #\n",
        "#       Epoch  48 Time    218.0 lr = 0.007501 avg loss = 0.003090 accuracy = 73.24  #\n",
        "#       Epoch  49 Time    217.6 lr = 0.005289 avg loss = 0.003071 accuracy = 73.48  #\n",
        "#       Epoch  50 Time    217.9 lr = 0.003467 avg loss = 0.003021 accuracy = 73.44  #\n",
        "#       Epoch  51 Time    218.6 lr = 0.002042 avg loss = 0.002991 accuracy = 73.52  #\n",
        "#       Epoch  52 Time    219.0 lr = 0.001020 avg loss = 0.002964 accuracy = 73.28  #\n",
        "#       Epoch  53 Time    219.5 lr = 0.000405 avg loss = 0.002954 accuracy = 73.50  #\n",
        "#       Epoch  54 Time    217.9 lr = 0.000200 avg loss = 0.002959 accuracy = 73.44  #\n",
        "#                                                                                   #\n",
        "#####################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNujOXllc7gf"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# IMPORT                                                                        #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn       as     nn\n",
        "import torch.optim    as     optim\n",
        "from   torch.autograd import Function\n",
        "\n",
        "# torch utils\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# additional libraries\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import time\n",
        "import math\n",
        "import numpy             as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpAXQHuzdEAj"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# PARAMETERS                                                                    #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# data\n",
        "DATA_DIR_1        = 'data'\n",
        "DATA_DIR_2        = 'data/imagenet64'\n",
        "DATA_DIR_TRAIN    = 'data/imagenet64/train'\n",
        "DATA_DIR_TEST     = 'data/imagenet64/val'\n",
        "DATA_FILE_TRAIN_1 = 'Train1.zip'\n",
        "DATA_FILE_TRAIN_2 = 'Train2.zip'\n",
        "DATA_FILE_TRAIN_3 = 'Train3.zip'\n",
        "DATA_FILE_TRAIN_4 = 'Train4.zip'\n",
        "DATA_FILE_TRAIN_5 = 'Train5.zip'\n",
        "DATA_FILE_TEST_1  = 'Val1.zip'\n",
        "DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'\n",
        "DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'\n",
        "DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'\n",
        "DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'\n",
        "DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'\n",
        "DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'\n",
        "DATA_BATCH_SIZE   = 256\n",
        "DATA_NUM_WORKERS  = 4\n",
        "DATA_NUM_CHANNELS = 3\n",
        "DATA_NUM_CLASSES  = 100\n",
        "DATA_RESIZE       = 64\n",
        "DATA_CROP         = 56\n",
        "DATA_MEAN         = (0.485, 0.456, 0.406)\n",
        "DATA_STD_DEV      = (0.229, 0.224, 0.225)\n",
        "\n",
        "# model\n",
        "MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation\n",
        "MODEL_LEVEL_2_BLOCKS   = 1\n",
        "MODEL_LEVEL_3_BLOCKS   = 2\n",
        "MODEL_LEVEL_4_BLOCKS   = 3\n",
        "MODEL_LEVEL_5_BLOCKS   = 4\n",
        "MODEL_LEVEL_1_CHANNELS = 16\n",
        "MODEL_LEVEL_2_CHANNELS = 24\n",
        "MODEL_LEVEL_3_CHANNELS = 40\n",
        "MODEL_LEVEL_4_CHANNELS = 80\n",
        "MODEL_LEVEL_5_CHANNELS = 160\n",
        "MODEL_HEAD_CONV_IN_CHANNELS = 320\n",
        "MODEL_HEAD_CONV_OUT_CHANNELS = 1280\n",
        "RANK_REDUCTION_RATIO = 4\n",
        "\n",
        "# training\n",
        "TRAIN_LR_MAX              = 0.2\n",
        "TRAIN_LR_INIT_SCALE       = 0.01\n",
        "TRAIN_LR_FINAL_SCALE      = 0.001\n",
        "TRAIN_LR_INIT_EPOCHS      = 5\n",
        "TRAIN_LR_FINAL_EPOCHS     = 50 # 100\n",
        "TRAIN_NUM_EPOCHS          = TRAIN_LR_INIT_EPOCHS + TRAIN_LR_FINAL_EPOCHS\n",
        "TRAIN_LR_INIT             = TRAIN_LR_MAX*TRAIN_LR_INIT_SCALE\n",
        "TRAIN_LR_FINAL            = TRAIN_LR_MAX*TRAIN_LR_FINAL_SCALE\n",
        "TRAIN_INTRA_EPOCH_DISPLAY = 10000\n",
        "\n",
        "# file\n",
        "FILE_NAME_CHECK      = 'EffNetStyleCheck.pt'\n",
        "FILE_NAME_BEST       = 'EffNetStyleBest.pt'\n",
        "FILE_SAVE            = True\n",
        "FILE_LOAD            = False\n",
        "FILE_EXTEND_TRAINING = False\n",
        "FILE_NEW_OPTIMIZER   = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr4253qTdJx9"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# DATA                                                                          #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# create a local directory structure for data storage\n",
        "if (os.path.exists(DATA_DIR_1) == False):\n",
        "    os.mkdir(DATA_DIR_1)\n",
        "if (os.path.exists(DATA_DIR_2) == False):\n",
        "    os.mkdir(DATA_DIR_2)\n",
        "if (os.path.exists(DATA_DIR_TRAIN) == False):\n",
        "    os.mkdir(DATA_DIR_TRAIN)\n",
        "if (os.path.exists(DATA_DIR_TEST) == False):\n",
        "    os.mkdir(DATA_DIR_TEST)\n",
        "\n",
        "# download data\n",
        "if (os.path.exists(DATA_FILE_TRAIN_1) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_2) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_3) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_4) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_5) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)\n",
        "if (os.path.exists(DATA_FILE_TEST_1) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)\n",
        "\n",
        "# extract data\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TEST)\n",
        "\n",
        "# transforms\n",
        "transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
        "transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
        "\n",
        "# data sets\n",
        "dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)\n",
        "dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)\n",
        "\n",
        "# data loader\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jML5Ry05KoF7"
      },
      "source": [
        "# Squeeze and Excite\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, Ne, R):\n",
        "      super(SqueezeExcitation, self).__init__()\n",
        "      out_filter = int(Ne/R)\n",
        "      self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)\n",
        "      self.flat = nn.Flatten()\n",
        "      self.linear1 = nn.Linear(Ne, out_filter, bias=True)\n",
        "      self.relu1 = nn.ReLU()\n",
        "      self.linear2 = nn.Linear(out_filter, Ne, bias=True)\n",
        "\n",
        "    def forward(self,x):\n",
        "      batch = x.shape[0]\n",
        "      ne = x.shape[1]\n",
        "      # print(\"x shape: \", x.shape)\n",
        "      out = self.avgpool(x)\n",
        "      # print(\"after global average pool: \", out.shape)\n",
        "      out = self.flat(out)\n",
        "      # print(\"after flatten: \", out.shape)\n",
        "      out = self.linear1(out)\n",
        "      # print(\"after linear1: \", out.shape)\n",
        "      out = self.relu1(out)\n",
        "      # print(\"after relu: \", out.shape)\n",
        "      out = self.linear2(out).sigmoid()\n",
        "      # print(\"after linear2 and sigmoid: \", out.shape)\n",
        "      out = out.reshape([batch, ne, 1,1])\n",
        "      # print(\"after reshape: \", out.shape)\n",
        "      out = x * out\n",
        "      return out\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpFqdJ4VdSI-"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# NETWORK BUILDING BLOCK                                                        #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# inverted residual block\n",
        "class InvResBlock(nn.Module):\n",
        "\n",
        "    # initialization\n",
        "    # Q: do we need Group Size?\n",
        "    def __init__(self, Ni, Ne, No, F, S, R):\n",
        "\n",
        "        # parent initialization\n",
        "        # create all of the operators for the inverted residual block in fig 2a\n",
        "        # of the paper; note that parameter names were chosen to match the paper\n",
        "        super(InvResBlock, self).__init__()\n",
        "\n",
        "        if ((Ni==No) and S==1):\n",
        "          self.id = True\n",
        "          # self.conv0 = nn.Conv2d(Ni, Ne, kernel_size=1, stride=S, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
        "          # self.bn = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        else:\n",
        "          self.id = False\n",
        "        \n",
        "        P = np.floor(F/2).astype(int)\n",
        "        self.conv1 = nn.Conv2d(Ni, Ne, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
        "        self.bn1   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "\n",
        "        self.conv2 = nn.Conv2d(Ne, Ne, kernel_size=F, stride=S, padding=P, dilation=1, groups=Ne, bias=False, padding_mode='zeros')\n",
        "        self.bn2   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.squeezeExcite = SqueezeExcitation(Ne, R)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(Ne, No, kernel_size=1,stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
        "        self.bn3   = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "        # sum\n",
        "        # self.relu0 = nn.ReLU()\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    # forward path\n",
        "    def forward(self, x):\n",
        "\n",
        "        # map input x to output y for the inverted residual block in fig 2a of\n",
        "        # the paper via connecting the operators defined in the initialization\n",
        "        # and return output y\n",
        "        if (self.id == True):\n",
        "          id = x\n",
        "\n",
        "        res = self.conv1(x)\n",
        "        res = self.bn1(res)\n",
        "        res = self.relu1(res)\n",
        "        res = self.conv2(res)\n",
        "        res = self.bn2(res)\n",
        "        res = self.relu2(res)\n",
        "        res = self.squeezeExcite(res)\n",
        "        res = self.conv3(res)\n",
        "        res = self.bn3(res)\n",
        "\n",
        "        y = res\n",
        "\n",
        "        if (self.id == True):\n",
        "          y = y+id\n",
        "        # y = self.relu0(y)\n",
        "\n",
        "        # return\n",
        "        return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD0RsAHbdZ65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8a6caf6-a905-4baf-bca6-de4ea569ebc9"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# NETWORK                                                                       #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# define\n",
        "class Model(nn.Module):\n",
        "\n",
        "    # initialization\n",
        "    # add necessary parameters to the init function to create the model defined\n",
        "    # in table 1 of the paper\n",
        "    def __init__(self, data_num_channels,\n",
        "                 model_level_1_blocks, model_level_1_channels,\n",
        "                 model_level_2_blocks, model_level_2_channels,\n",
        "                 model_level_3_blocks, model_level_3_channels,\n",
        "                 model_level_4_blocks, model_level_4_channels,\n",
        "                 model_level_5_blocks, model_level_5_channels,\n",
        "                 model_head_conv_in_channels, model_head_conv_out_channels, data_num_classes, rank_reduction_ratio): \n",
        "\n",
        "        # parent initialization\n",
        "        super(Model, self).__init__()\n",
        "        # create all of the operators for the network defined in table 1 of the\n",
        "        # paper using a combination of Python, standard PyTorch operators and\n",
        "        # the previously defined InvResBlock class\n",
        "        stride1 = 1\n",
        "        stride2 = 2\n",
        "        stride3 = 2\n",
        "        stride4 = 2\n",
        "        stride5 = 1\n",
        "\n",
        "\n",
        "        # MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation\n",
        "        # MODEL_LEVEL_2_BLOCKS   = 1\n",
        "        # MODEL_LEVEL_3_BLOCKS   = 2\n",
        "        # MODEL_LEVEL_4_BLOCKS   = 3\n",
        "        # MODEL_LEVEL_5_BLOCKS   = 4\n",
        "        # MODEL_LEVEL_1_CHANNELS = 16\n",
        "        # MODEL_LEVEL_2_CHANNELS = 24\n",
        "        # MODEL_LEVEL_3_CHANNELS = 40\n",
        "        # MODEL_LEVEL_4_CHANNELS = 80\n",
        "        # MODEL_LEVEL_5_CHANNELS = 160\n",
        "\n",
        "        #Tail\n",
        "        self.tail = nn.ModuleList()\n",
        "        self.tail.append(nn.Conv2d(data_num_channels, model_level_1_channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros'))\n",
        "        self.tail.append(nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
        "        self.tail.append(nn.ReLU())\n",
        "\n",
        "        # encoder level 1-Body\n",
        "        self.enc_1 = nn.ModuleList()\n",
        "        for n in range(model_level_1_blocks - 1):\n",
        "            self.enc_1.append(InvResBlock(model_level_1_channels, 4*model_level_1_channels, model_level_1_channels, 3, 1, rank_reduction_ratio))\n",
        "        self.enc_1.append(InvResBlock(model_level_1_channels, 4*model_level_1_channels, model_level_2_channels, 3, stride1, rank_reduction_ratio))\n",
        "\n",
        "        # encoder level 2-Body\n",
        "        self.enc_2 = nn.ModuleList()\n",
        "        for n in range(model_level_2_blocks - 1):\n",
        "            self.enc_2.append(InvResBlock(model_level_2_channels, 4*model_level_2_channels, model_level_2_channels, 3, 1, rank_reduction_ratio))\n",
        "        self.enc_2.append(InvResBlock(model_level_2_channels, 4*model_level_2_channels, model_level_3_channels, 3, stride2, rank_reduction_ratio))\n",
        "\n",
        "        # encoder level 3-Body\n",
        "        self.enc_3 = nn.ModuleList()\n",
        "        for n in range(model_level_3_blocks - 1):\n",
        "            self.enc_3.append(InvResBlock(model_level_3_channels, 4*model_level_3_channels, model_level_3_channels, 3, 1, rank_reduction_ratio))\n",
        "        self.enc_3.append(InvResBlock(model_level_3_channels, 4*model_level_3_channels, model_level_4_channels, 3, stride3, rank_reduction_ratio))\n",
        "\n",
        "        # encoder level 4-Body\n",
        "        self.enc_4 = nn.ModuleList()\n",
        "        for n in range(model_level_4_blocks - 1):\n",
        "            self.enc_4.append(InvResBlock(model_level_4_channels, 4*model_level_4_channels, model_level_4_channels, 3, 1, rank_reduction_ratio))\n",
        "        self.enc_4.append(InvResBlock(model_level_4_channels, 4*model_level_4_channels, model_level_5_channels, 3, stride4, rank_reduction_ratio))\n",
        "\n",
        "        # encoder level 5-Body\n",
        "        self.enc_5 = nn.ModuleList()\n",
        "        for n in range(model_level_5_blocks - 1):\n",
        "            self.enc_5.append(InvResBlock(model_level_5_channels, 4*model_level_5_channels, model_level_5_channels, 3, 1, rank_reduction_ratio))\n",
        "        self.enc_5.append(InvResBlock(model_level_5_channels, 4*model_level_5_channels, model_head_conv_in_channels, 3, stride5, rank_reduction_ratio))\n",
        "\n",
        "        # decoder-Head\n",
        "        self.dec = nn.ModuleList()\n",
        "        self.dec.append(nn.Conv2d(model_head_conv_in_channels,model_head_conv_out_channels, kernel_size = 1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros'))\n",
        "        self.dec.append(nn.BatchNorm2d(model_head_conv_out_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
        "        self.dec.append(nn.ReLU())\n",
        "        self.dec.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "        self.dec.append(nn.Flatten())\n",
        "        self.dec.append(nn.Linear(model_head_conv_out_channels, data_num_classes, bias=True))\n",
        "\n",
        "    # forward path\n",
        "    def forward(self, x):\n",
        "\n",
        "        # map input x to output y for the network defined in table 1 of the\n",
        "        # paper via connecting the operators defined in the initialization\n",
        "        # and return output y\n",
        "\n",
        "        for layer in self.tail:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 1\n",
        "        for layer in self.enc_1:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 2\n",
        "        for layer in self.enc_2:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 3\n",
        "        for layer in self.enc_3:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 4\n",
        "        for layer in self.enc_4:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 5\n",
        "        for layer in self.enc_5:\n",
        "            x = layer(x)\n",
        "\n",
        "        # decoder\n",
        "        for layer in self.dec:\n",
        "            x = layer(x)\n",
        "\n",
        "\n",
        "        # return\n",
        "        return x\n",
        "\n",
        "# create\n",
        "# add necessary parameters to the init function to create the model defined\n",
        "# in table 1 of the paper\n",
        "model = Model(DATA_NUM_CHANNELS,\n",
        "              MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_1_CHANNELS,\n",
        "              MODEL_LEVEL_2_BLOCKS, MODEL_LEVEL_2_CHANNELS,\n",
        "              MODEL_LEVEL_3_BLOCKS, MODEL_LEVEL_3_CHANNELS,\n",
        "              MODEL_LEVEL_4_BLOCKS, MODEL_LEVEL_4_CHANNELS,\n",
        "              MODEL_LEVEL_5_BLOCKS, MODEL_LEVEL_5_CHANNELS,\n",
        "              MODEL_HEAD_CONV_IN_CHANNELS, MODEL_HEAD_CONV_OUT_CHANNELS, DATA_NUM_CLASSES, RANK_REDUCTION_RATIO) \n",
        "\n",
        "# enable data parallelization for multi GPU systems\n",
        "if (torch.cuda.device_count() > 1):\n",
        "    model = nn.DataParallel(model)\n",
        "print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1 GPU(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vePm7dKwdi5A"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# ERROR AND OPTIMIZER                                                           #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# error (softmax cross entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "\n",
        "    # linear warmup followed by 1/2 wave cosine decay\n",
        "    if epoch < TRAIN_LR_INIT_EPOCHS:\n",
        "        lr = (TRAIN_LR_MAX - TRAIN_LR_INIT)*(float(epoch)/TRAIN_LR_INIT_EPOCHS) + TRAIN_LR_INIT\n",
        "    else:\n",
        "        lr = TRAIN_LR_FINAL + 0.5*(TRAIN_LR_MAX - TRAIN_LR_FINAL)*(1.0 + math.cos(((float(epoch) - TRAIN_LR_INIT_EPOCHS)/(TRAIN_LR_FINAL_EPOCHS - 1.0))*math.pi))\n",
        "\n",
        "    return lr\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, dampening=0.0, weight_decay=5e-5, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LWCL0HhdnWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e32a34ca-76ae-4a07-c917-4edafbe9ca87"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# TRAINING                                                                      #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# start epoch\n",
        "start_epoch = 0\n",
        "\n",
        "# specify the device as the GPU if present with fallback to the CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# transfer the network to the device\n",
        "model.to(device)\n",
        "\n",
        "# load the last checkpoint\n",
        "if (FILE_LOAD == True):\n",
        "    checkpoint = torch.load(FILE_NAME_CHECK)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if (FILE_NEW_OPTIMIZER == False):\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if (FILE_EXTEND_TRAINING == False):\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "# initialize the epoch\n",
        "accuracy_best      = 0\n",
        "start_time_display = time.time()\n",
        "start_time_epoch   = time.time()\n",
        "\n",
        "# cycle through the epochs\n",
        "for epoch in range(start_epoch, TRAIN_NUM_EPOCHS):\n",
        "\n",
        "    # initialize epoch training\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    num_batches   = 0\n",
        "    num_display   = 0\n",
        "\n",
        "    # set the learning rate for the epoch\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr_schedule(epoch)\n",
        "\n",
        "    # cycle through the training data set\n",
        "    for data in dataloader_train:\n",
        "\n",
        "        # extract a batch of data and move it to the appropriate device\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass, loss, backward pass and weight update\n",
        "        outputs = model(inputs)\n",
        "        loss    = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update statistics\n",
        "        training_loss = training_loss + loss.item()\n",
        "        num_batches   = num_batches + 1\n",
        "        num_display   = num_display + DATA_BATCH_SIZE\n",
        "\n",
        "        # display intra epoch results\n",
        "        if (num_display > TRAIN_INTRA_EPOCH_DISPLAY):\n",
        "            num_display          = 0\n",
        "            elapsed_time_display = time.time() - start_time_display\n",
        "            start_time_display   = time.time()\n",
        "            print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f}'.format(epoch, elapsed_time_display, lr_schedule(epoch), (training_loss / num_batches) / DATA_BATCH_SIZE), flush=True)\n",
        "\n",
        "    # initialize epoch testing\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total   = 0\n",
        "\n",
        "    # no weight update / no gradient needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # cycle through the testing data set\n",
        "        for data in dataloader_test:\n",
        "\n",
        "            # extract a batch of data and move it to the appropriate device\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass and prediction\n",
        "            outputs      = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # update test set statistics\n",
        "            test_total   = test_total + labels.size(0)\n",
        "            test_correct = test_correct + (predicted == labels).sum().item()\n",
        "\n",
        "    # epoch statistics\n",
        "    elapsed_time_epoch = time.time() - start_time_epoch\n",
        "    start_time_epoch   = time.time()\n",
        "    print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f} accuracy = {4:5.2f}'.format(epoch, elapsed_time_epoch, lr_schedule(epoch), (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total)), flush=True)\n",
        "\n",
        "    # save a checkpoint\n",
        "    if (FILE_SAVE == True):\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_CHECK)\n",
        "\n",
        "    # save the best model\n",
        "    accuracy_epoch = 100.0 * test_correct / test_total\n",
        "    if ((FILE_SAVE == True) and (accuracy_epoch >= accuracy_best)):\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_BEST)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Time     16.9 lr = 0.002000 avg loss = 0.018007\n",
            "Epoch   0 Time     16.9 lr = 0.002000 avg loss = 0.017923\n",
            "Epoch   0 Time     16.8 lr = 0.002000 avg loss = 0.017837\n",
            "Epoch   0 Time     16.9 lr = 0.002000 avg loss = 0.017736\n",
            "Epoch   0 Time     16.9 lr = 0.002000 avg loss = 0.017614\n",
            "Epoch   0 Time     17.0 lr = 0.002000 avg loss = 0.017501\n",
            "Epoch   0 Time     16.8 lr = 0.002000 avg loss = 0.017374\n",
            "Epoch   0 Time     16.9 lr = 0.002000 avg loss = 0.017239\n",
            "Epoch   0 Time     16.9 lr = 0.002000 avg loss = 0.017109\n",
            "Epoch   0 Time     16.8 lr = 0.002000 avg loss = 0.016987\n",
            "Epoch   0 Time     17.0 lr = 0.002000 avg loss = 0.016865\n",
            "Epoch   0 Time     16.8 lr = 0.002000 avg loss = 0.016752\n",
            "Epoch   0 Time    217.1 lr = 0.002000 avg loss = 0.016681 accuracy =  9.52\n",
            "Epoch   1 Time     31.6 lr = 0.041600 avg loss = 0.015358\n",
            "Epoch   1 Time     16.8 lr = 0.041600 avg loss = 0.014962\n",
            "Epoch   1 Time     16.8 lr = 0.041600 avg loss = 0.014647\n",
            "Epoch   1 Time     16.8 lr = 0.041600 avg loss = 0.014376\n",
            "Epoch   1 Time     16.7 lr = 0.041600 avg loss = 0.014113\n",
            "Epoch   1 Time     16.7 lr = 0.041600 avg loss = 0.013891\n",
            "Epoch   1 Time     16.7 lr = 0.041600 avg loss = 0.013693\n",
            "Epoch   1 Time     16.8 lr = 0.041600 avg loss = 0.013501\n",
            "Epoch   1 Time     16.6 lr = 0.041600 avg loss = 0.013329\n",
            "Epoch   1 Time     16.7 lr = 0.041600 avg loss = 0.013166\n",
            "Epoch   1 Time     16.8 lr = 0.041600 avg loss = 0.013012\n",
            "Epoch   1 Time     16.8 lr = 0.041600 avg loss = 0.012865\n",
            "Epoch   1 Time    215.3 lr = 0.041600 avg loss = 0.012784 accuracy = 29.42\n",
            "Epoch   2 Time     30.9 lr = 0.081200 avg loss = 0.011700\n",
            "Epoch   2 Time     16.7 lr = 0.081200 avg loss = 0.011559\n",
            "Epoch   2 Time     16.7 lr = 0.081200 avg loss = 0.011458\n",
            "Epoch   2 Time     16.8 lr = 0.081200 avg loss = 0.011337\n",
            "Epoch   2 Time     16.8 lr = 0.081200 avg loss = 0.011254\n",
            "Epoch   2 Time     16.7 lr = 0.081200 avg loss = 0.011162\n",
            "Epoch   2 Time     16.8 lr = 0.081200 avg loss = 0.011051\n",
            "Epoch   2 Time     16.7 lr = 0.081200 avg loss = 0.010959\n",
            "Epoch   2 Time     16.9 lr = 0.081200 avg loss = 0.010879\n",
            "Epoch   2 Time     16.6 lr = 0.081200 avg loss = 0.010813\n",
            "Epoch   2 Time     16.7 lr = 0.081200 avg loss = 0.010734\n",
            "Epoch   2 Time     16.7 lr = 0.081200 avg loss = 0.010669\n",
            "Epoch   2 Time    215.1 lr = 0.081200 avg loss = 0.010624 accuracy = 36.74\n",
            "Epoch   3 Time     31.1 lr = 0.120800 avg loss = 0.009986\n",
            "Epoch   3 Time     16.7 lr = 0.120800 avg loss = 0.010034\n",
            "Epoch   3 Time     16.8 lr = 0.120800 avg loss = 0.009993\n",
            "Epoch   3 Time     16.8 lr = 0.120800 avg loss = 0.009949\n",
            "Epoch   3 Time     16.5 lr = 0.120800 avg loss = 0.009908\n",
            "Epoch   3 Time     16.8 lr = 0.120800 avg loss = 0.009845\n",
            "Epoch   3 Time     16.6 lr = 0.120800 avg loss = 0.009780\n",
            "Epoch   3 Time     16.8 lr = 0.120800 avg loss = 0.009737\n",
            "Epoch   3 Time     16.4 lr = 0.120800 avg loss = 0.009689\n",
            "Epoch   3 Time     16.5 lr = 0.120800 avg loss = 0.009645\n",
            "Epoch   3 Time     16.8 lr = 0.120800 avg loss = 0.009605\n",
            "Epoch   3 Time     16.7 lr = 0.120800 avg loss = 0.009564\n",
            "Epoch   3 Time    214.6 lr = 0.120800 avg loss = 0.009530 accuracy = 41.72\n",
            "Epoch   4 Time     31.2 lr = 0.160400 avg loss = 0.009109\n",
            "Epoch   4 Time     16.6 lr = 0.160400 avg loss = 0.009106\n",
            "Epoch   4 Time     16.7 lr = 0.160400 avg loss = 0.009073\n",
            "Epoch   4 Time     16.6 lr = 0.160400 avg loss = 0.009038\n",
            "Epoch   4 Time     16.8 lr = 0.160400 avg loss = 0.009021\n",
            "Epoch   4 Time     16.6 lr = 0.160400 avg loss = 0.008990\n",
            "Epoch   4 Time     16.7 lr = 0.160400 avg loss = 0.008960\n",
            "Epoch   4 Time     16.9 lr = 0.160400 avg loss = 0.008924\n",
            "Epoch   4 Time     16.8 lr = 0.160400 avg loss = 0.008889\n",
            "Epoch   4 Time     16.7 lr = 0.160400 avg loss = 0.008866\n",
            "Epoch   4 Time     16.9 lr = 0.160400 avg loss = 0.008834\n",
            "Epoch   4 Time     16.7 lr = 0.160400 avg loss = 0.008807\n",
            "Epoch   4 Time    215.4 lr = 0.160400 avg loss = 0.008793 accuracy = 43.56\n",
            "Epoch   5 Time     31.5 lr = 0.200000 avg loss = 0.008375\n",
            "Epoch   5 Time     17.0 lr = 0.200000 avg loss = 0.008415\n",
            "Epoch   5 Time     16.9 lr = 0.200000 avg loss = 0.008452\n",
            "Epoch   5 Time     17.0 lr = 0.200000 avg loss = 0.008435\n",
            "Epoch   5 Time     16.9 lr = 0.200000 avg loss = 0.008404\n",
            "Epoch   5 Time     17.0 lr = 0.200000 avg loss = 0.008389\n",
            "Epoch   5 Time     16.9 lr = 0.200000 avg loss = 0.008363\n",
            "Epoch   5 Time     16.9 lr = 0.200000 avg loss = 0.008335\n",
            "Epoch   5 Time     16.8 lr = 0.200000 avg loss = 0.008318\n",
            "Epoch   5 Time     17.0 lr = 0.200000 avg loss = 0.008321\n",
            "Epoch   5 Time     17.0 lr = 0.200000 avg loss = 0.008294\n",
            "Epoch   5 Time     17.0 lr = 0.200000 avg loss = 0.008276\n",
            "Epoch   5 Time    218.1 lr = 0.200000 avg loss = 0.008264 accuracy = 47.42\n",
            "Epoch   6 Time     31.9 lr = 0.199795 avg loss = 0.007756\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007772\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007833\n",
            "Epoch   6 Time     16.9 lr = 0.199795 avg loss = 0.007848\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007785\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007780\n",
            "Epoch   6 Time     17.1 lr = 0.199795 avg loss = 0.007774\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007766\n",
            "Epoch   6 Time     17.1 lr = 0.199795 avg loss = 0.007745\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007724\n",
            "Epoch   6 Time     17.2 lr = 0.199795 avg loss = 0.007717\n",
            "Epoch   6 Time     17.0 lr = 0.199795 avg loss = 0.007698\n",
            "Epoch   6 Time    219.1 lr = 0.199795 avg loss = 0.007689 accuracy = 49.68\n",
            "Epoch   7 Time     31.7 lr = 0.199180 avg loss = 0.007304\n",
            "Epoch   7 Time     17.1 lr = 0.199180 avg loss = 0.007356\n",
            "Epoch   7 Time     17.1 lr = 0.199180 avg loss = 0.007378\n",
            "Epoch   7 Time     17.1 lr = 0.199180 avg loss = 0.007343\n",
            "Epoch   7 Time     17.2 lr = 0.199180 avg loss = 0.007344\n",
            "Epoch   7 Time     17.1 lr = 0.199180 avg loss = 0.007331\n",
            "Epoch   7 Time     17.2 lr = 0.199180 avg loss = 0.007322\n",
            "Epoch   7 Time     17.2 lr = 0.199180 avg loss = 0.007329\n",
            "Epoch   7 Time     17.1 lr = 0.199180 avg loss = 0.007338\n",
            "Epoch   7 Time     17.1 lr = 0.199180 avg loss = 0.007317\n",
            "Epoch   7 Time     17.2 lr = 0.199180 avg loss = 0.007300\n",
            "Epoch   7 Time     17.2 lr = 0.199180 avg loss = 0.007295\n",
            "Epoch   7 Time    220.5 lr = 0.199180 avg loss = 0.007285 accuracy = 54.36\n",
            "Epoch   8 Time     32.0 lr = 0.198158 avg loss = 0.007080\n",
            "Epoch   8 Time     17.2 lr = 0.198158 avg loss = 0.007046\n",
            "Epoch   8 Time     17.3 lr = 0.198158 avg loss = 0.007082\n",
            "Epoch   8 Time     17.2 lr = 0.198158 avg loss = 0.007054\n",
            "Epoch   8 Time     17.1 lr = 0.198158 avg loss = 0.007037\n",
            "Epoch   8 Time     17.2 lr = 0.198158 avg loss = 0.007028\n",
            "Epoch   8 Time     17.2 lr = 0.198158 avg loss = 0.007018\n",
            "Epoch   8 Time     17.2 lr = 0.198158 avg loss = 0.007017\n",
            "Epoch   8 Time     17.3 lr = 0.198158 avg loss = 0.007006\n",
            "Epoch   8 Time     17.4 lr = 0.198158 avg loss = 0.007006\n",
            "Epoch   8 Time     17.3 lr = 0.198158 avg loss = 0.007001\n",
            "Epoch   8 Time     17.3 lr = 0.198158 avg loss = 0.007004\n",
            "Epoch   8 Time    221.6 lr = 0.198158 avg loss = 0.006996 accuracy = 54.22\n",
            "Epoch   9 Time     32.1 lr = 0.196733 avg loss = 0.006711\n",
            "Epoch   9 Time     17.1 lr = 0.196733 avg loss = 0.006761\n",
            "Epoch   9 Time     17.0 lr = 0.196733 avg loss = 0.006755\n",
            "Epoch   9 Time     17.1 lr = 0.196733 avg loss = 0.006732\n",
            "Epoch   9 Time     17.0 lr = 0.196733 avg loss = 0.006739\n",
            "Epoch   9 Time     17.0 lr = 0.196733 avg loss = 0.006720\n",
            "Epoch   9 Time     17.1 lr = 0.196733 avg loss = 0.006746\n",
            "Epoch   9 Time     17.1 lr = 0.196733 avg loss = 0.006761\n",
            "Epoch   9 Time     17.2 lr = 0.196733 avg loss = 0.006762\n",
            "Epoch   9 Time     17.1 lr = 0.196733 avg loss = 0.006751\n",
            "Epoch   9 Time     17.1 lr = 0.196733 avg loss = 0.006754\n",
            "Epoch   9 Time     17.2 lr = 0.196733 avg loss = 0.006758\n",
            "Epoch   9 Time    220.2 lr = 0.196733 avg loss = 0.006755 accuracy = 56.78\n",
            "Epoch  10 Time     31.9 lr = 0.194911 avg loss = 0.006558\n",
            "Epoch  10 Time     17.1 lr = 0.194911 avg loss = 0.006550\n",
            "Epoch  10 Time     17.1 lr = 0.194911 avg loss = 0.006536\n",
            "Epoch  10 Time     17.1 lr = 0.194911 avg loss = 0.006538\n",
            "Epoch  10 Time     16.8 lr = 0.194911 avg loss = 0.006557\n",
            "Epoch  10 Time     16.6 lr = 0.194911 avg loss = 0.006563\n",
            "Epoch  10 Time     16.7 lr = 0.194911 avg loss = 0.006560\n",
            "Epoch  10 Time     16.6 lr = 0.194911 avg loss = 0.006560\n",
            "Epoch  10 Time     16.6 lr = 0.194911 avg loss = 0.006551\n",
            "Epoch  10 Time     17.1 lr = 0.194911 avg loss = 0.006550\n",
            "Epoch  10 Time     17.2 lr = 0.194911 avg loss = 0.006553\n",
            "Epoch  10 Time     16.6 lr = 0.194911 avg loss = 0.006552\n",
            "Epoch  10 Time    217.0 lr = 0.194911 avg loss = 0.006551 accuracy = 56.16\n",
            "Epoch  11 Time     31.2 lr = 0.192699 avg loss = 0.006293\n",
            "Epoch  11 Time     16.6 lr = 0.192699 avg loss = 0.006317\n",
            "Epoch  11 Time     16.6 lr = 0.192699 avg loss = 0.006353\n",
            "Epoch  11 Time     16.5 lr = 0.192699 avg loss = 0.006347\n",
            "Epoch  11 Time     16.5 lr = 0.192699 avg loss = 0.006370\n",
            "Epoch  11 Time     16.7 lr = 0.192699 avg loss = 0.006363\n",
            "Epoch  11 Time     16.7 lr = 0.192699 avg loss = 0.006372\n",
            "Epoch  11 Time     16.6 lr = 0.192699 avg loss = 0.006369\n",
            "Epoch  11 Time     16.7 lr = 0.192699 avg loss = 0.006363\n",
            "Epoch  11 Time     16.6 lr = 0.192699 avg loss = 0.006363\n",
            "Epoch  11 Time     16.7 lr = 0.192699 avg loss = 0.006359\n",
            "Epoch  11 Time     16.7 lr = 0.192699 avg loss = 0.006361\n",
            "Epoch  11 Time    214.1 lr = 0.192699 avg loss = 0.006363 accuracy = 59.78\n",
            "Epoch  12 Time     31.1 lr = 0.190107 avg loss = 0.006108\n",
            "Epoch  12 Time     16.7 lr = 0.190107 avg loss = 0.006144\n",
            "Epoch  12 Time     16.6 lr = 0.190107 avg loss = 0.006172\n",
            "Epoch  12 Time     16.8 lr = 0.190107 avg loss = 0.006189\n",
            "Epoch  12 Time     16.6 lr = 0.190107 avg loss = 0.006214\n",
            "Epoch  12 Time     16.7 lr = 0.190107 avg loss = 0.006210\n",
            "Epoch  12 Time     16.8 lr = 0.190107 avg loss = 0.006225\n",
            "Epoch  12 Time     16.6 lr = 0.190107 avg loss = 0.006243\n",
            "Epoch  12 Time     16.7 lr = 0.190107 avg loss = 0.006227\n",
            "Epoch  12 Time     16.6 lr = 0.190107 avg loss = 0.006212\n",
            "Epoch  12 Time     16.8 lr = 0.190107 avg loss = 0.006203\n",
            "Epoch  12 Time     16.8 lr = 0.190107 avg loss = 0.006206\n",
            "Epoch  12 Time    214.8 lr = 0.190107 avg loss = 0.006212 accuracy = 60.14\n",
            "Epoch  13 Time     31.1 lr = 0.187145 avg loss = 0.005980\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006017\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006034\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006053\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006065\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006069\n",
            "Epoch  13 Time     16.8 lr = 0.187145 avg loss = 0.006058\n",
            "Epoch  13 Time     16.8 lr = 0.187145 avg loss = 0.006057\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006048\n",
            "Epoch  13 Time     16.6 lr = 0.187145 avg loss = 0.006064\n",
            "Epoch  13 Time     16.7 lr = 0.187145 avg loss = 0.006063\n",
            "Epoch  13 Time     16.8 lr = 0.187145 avg loss = 0.006055\n",
            "Epoch  13 Time    215.3 lr = 0.187145 avg loss = 0.006057 accuracy = 61.98\n",
            "Epoch  14 Time     31.4 lr = 0.183825 avg loss = 0.005934\n",
            "Epoch  14 Time     16.7 lr = 0.183825 avg loss = 0.005867\n",
            "Epoch  14 Time     16.8 lr = 0.183825 avg loss = 0.005894\n",
            "Epoch  14 Time     16.9 lr = 0.183825 avg loss = 0.005921\n",
            "Epoch  14 Time     16.8 lr = 0.183825 avg loss = 0.005920\n",
            "Epoch  14 Time     16.7 lr = 0.183825 avg loss = 0.005923\n",
            "Epoch  14 Time     16.8 lr = 0.183825 avg loss = 0.005921\n",
            "Epoch  14 Time     16.7 lr = 0.183825 avg loss = 0.005921\n",
            "Epoch  14 Time     16.8 lr = 0.183825 avg loss = 0.005925\n",
            "Epoch  14 Time     16.9 lr = 0.183825 avg loss = 0.005931\n",
            "Epoch  14 Time     16.9 lr = 0.183825 avg loss = 0.005938\n",
            "Epoch  14 Time     16.8 lr = 0.183825 avg loss = 0.005942\n",
            "Epoch  14 Time    216.2 lr = 0.183825 avg loss = 0.005940 accuracy = 60.86\n",
            "Epoch  15 Time     31.7 lr = 0.180161 avg loss = 0.005750\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005736\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005769\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005784\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005762\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005769\n",
            "Epoch  15 Time     16.8 lr = 0.180161 avg loss = 0.005771\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005774\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005788\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005801\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005809\n",
            "Epoch  15 Time     16.9 lr = 0.180161 avg loss = 0.005806\n",
            "Epoch  15 Time    217.2 lr = 0.180161 avg loss = 0.005808 accuracy = 61.92\n",
            "Epoch  16 Time     31.5 lr = 0.176168 avg loss = 0.005632\n",
            "Epoch  16 Time     16.8 lr = 0.176168 avg loss = 0.005655\n",
            "Epoch  16 Time     16.9 lr = 0.176168 avg loss = 0.005693\n",
            "Epoch  16 Time     16.7 lr = 0.176168 avg loss = 0.005718\n",
            "Epoch  16 Time     16.8 lr = 0.176168 avg loss = 0.005734\n",
            "Epoch  16 Time     16.8 lr = 0.176168 avg loss = 0.005734\n",
            "Epoch  16 Time     16.8 lr = 0.176168 avg loss = 0.005722\n",
            "Epoch  16 Time     16.7 lr = 0.176168 avg loss = 0.005715\n",
            "Epoch  16 Time     16.8 lr = 0.176168 avg loss = 0.005714\n",
            "Epoch  16 Time     16.7 lr = 0.176168 avg loss = 0.005706\n",
            "Epoch  16 Time     16.9 lr = 0.176168 avg loss = 0.005703\n",
            "Epoch  16 Time     16.8 lr = 0.176168 avg loss = 0.005708\n",
            "Epoch  16 Time    216.2 lr = 0.176168 avg loss = 0.005708 accuracy = 63.24\n",
            "Epoch  17 Time     31.4 lr = 0.171863 avg loss = 0.005428\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005462\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005506\n",
            "Epoch  17 Time     16.9 lr = 0.171863 avg loss = 0.005550\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005570\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005571\n",
            "Epoch  17 Time     16.7 lr = 0.171863 avg loss = 0.005570\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005585\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005594\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005592\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005594\n",
            "Epoch  17 Time     16.8 lr = 0.171863 avg loss = 0.005601\n",
            "Epoch  17 Time    216.3 lr = 0.171863 avg loss = 0.005597 accuracy = 62.36\n",
            "Epoch  18 Time     31.3 lr = 0.167263 avg loss = 0.005506\n",
            "Epoch  18 Time     16.8 lr = 0.167263 avg loss = 0.005559\n",
            "Epoch  18 Time     17.0 lr = 0.167263 avg loss = 0.005566\n",
            "Epoch  18 Time     16.9 lr = 0.167263 avg loss = 0.005551\n",
            "Epoch  18 Time     16.9 lr = 0.167263 avg loss = 0.005563\n",
            "Epoch  18 Time     16.9 lr = 0.167263 avg loss = 0.005545\n",
            "Epoch  18 Time     17.0 lr = 0.167263 avg loss = 0.005522\n",
            "Epoch  18 Time     16.9 lr = 0.167263 avg loss = 0.005526\n",
            "Epoch  18 Time     16.9 lr = 0.167263 avg loss = 0.005530\n",
            "Epoch  18 Time     17.0 lr = 0.167263 avg loss = 0.005522\n",
            "Epoch  18 Time     17.0 lr = 0.167263 avg loss = 0.005527\n",
            "Epoch  18 Time     17.0 lr = 0.167263 avg loss = 0.005535\n",
            "Epoch  18 Time    218.0 lr = 0.167263 avg loss = 0.005533 accuracy = 64.68\n",
            "Epoch  19 Time     31.9 lr = 0.162387 avg loss = 0.005387\n",
            "Epoch  19 Time     17.0 lr = 0.162387 avg loss = 0.005367\n",
            "Epoch  19 Time     17.1 lr = 0.162387 avg loss = 0.005329\n",
            "Epoch  19 Time     17.1 lr = 0.162387 avg loss = 0.005362\n",
            "Epoch  19 Time     17.1 lr = 0.162387 avg loss = 0.005371\n",
            "Epoch  19 Time     17.1 lr = 0.162387 avg loss = 0.005396\n",
            "Epoch  19 Time     17.1 lr = 0.162387 avg loss = 0.005411\n",
            "Epoch  19 Time     17.1 lr = 0.162387 avg loss = 0.005408\n",
            "Epoch  19 Time     17.3 lr = 0.162387 avg loss = 0.005419\n",
            "Epoch  19 Time     17.2 lr = 0.162387 avg loss = 0.005418\n",
            "Epoch  19 Time     17.2 lr = 0.162387 avg loss = 0.005428\n",
            "Epoch  19 Time     17.0 lr = 0.162387 avg loss = 0.005431\n",
            "Epoch  19 Time    220.0 lr = 0.162387 avg loss = 0.005435 accuracy = 60.16\n",
            "Epoch  20 Time     31.9 lr = 0.157254 avg loss = 0.005313\n",
            "Epoch  20 Time     17.3 lr = 0.157254 avg loss = 0.005351\n",
            "Epoch  20 Time     17.3 lr = 0.157254 avg loss = 0.005305\n",
            "Epoch  20 Time     17.3 lr = 0.157254 avg loss = 0.005314\n",
            "Epoch  20 Time     17.2 lr = 0.157254 avg loss = 0.005325\n",
            "Epoch  20 Time     17.2 lr = 0.157254 avg loss = 0.005333\n",
            "Epoch  20 Time     17.2 lr = 0.157254 avg loss = 0.005353\n",
            "Epoch  20 Time     17.3 lr = 0.157254 avg loss = 0.005367\n",
            "Epoch  20 Time     17.2 lr = 0.157254 avg loss = 0.005368\n",
            "Epoch  20 Time     17.2 lr = 0.157254 avg loss = 0.005363\n",
            "Epoch  20 Time     17.3 lr = 0.157254 avg loss = 0.005362\n",
            "Epoch  20 Time     17.3 lr = 0.157254 avg loss = 0.005357\n",
            "Epoch  20 Time    221.7 lr = 0.157254 avg loss = 0.005361 accuracy = 65.94\n",
            "Epoch  21 Time     32.3 lr = 0.151887 avg loss = 0.005245\n",
            "Epoch  21 Time     17.4 lr = 0.151887 avg loss = 0.005205\n",
            "Epoch  21 Time     17.3 lr = 0.151887 avg loss = 0.005172\n",
            "Epoch  21 Time     17.3 lr = 0.151887 avg loss = 0.005187\n",
            "Epoch  21 Time     17.3 lr = 0.151887 avg loss = 0.005183\n",
            "Epoch  21 Time     17.4 lr = 0.151887 avg loss = 0.005200\n",
            "Epoch  21 Time     17.3 lr = 0.151887 avg loss = 0.005218\n",
            "Epoch  21 Time     17.0 lr = 0.151887 avg loss = 0.005221\n",
            "Epoch  21 Time     17.2 lr = 0.151887 avg loss = 0.005238\n",
            "Epoch  21 Time     17.1 lr = 0.151887 avg loss = 0.005242\n",
            "Epoch  21 Time     17.1 lr = 0.151887 avg loss = 0.005244\n",
            "Epoch  21 Time     17.2 lr = 0.151887 avg loss = 0.005251\n",
            "Epoch  21 Time    221.9 lr = 0.151887 avg loss = 0.005259 accuracy = 64.92\n",
            "Epoch  22 Time     32.1 lr = 0.146308 avg loss = 0.005068\n",
            "Epoch  22 Time     17.2 lr = 0.146308 avg loss = 0.005122\n",
            "Epoch  22 Time     17.2 lr = 0.146308 avg loss = 0.005119\n",
            "Epoch  22 Time     17.1 lr = 0.146308 avg loss = 0.005141\n",
            "Epoch  22 Time     17.1 lr = 0.146308 avg loss = 0.005126\n",
            "Epoch  22 Time     17.2 lr = 0.146308 avg loss = 0.005134\n",
            "Epoch  22 Time     17.3 lr = 0.146308 avg loss = 0.005145\n",
            "Epoch  22 Time     17.2 lr = 0.146308 avg loss = 0.005157\n",
            "Epoch  22 Time     17.2 lr = 0.146308 avg loss = 0.005153\n",
            "Epoch  22 Time     17.1 lr = 0.146308 avg loss = 0.005153\n",
            "Epoch  22 Time     17.0 lr = 0.146308 avg loss = 0.005161\n",
            "Epoch  22 Time     17.0 lr = 0.146308 avg loss = 0.005161\n",
            "Epoch  22 Time    220.5 lr = 0.146308 avg loss = 0.005167 accuracy = 64.82\n",
            "Epoch  23 Time     31.5 lr = 0.140538 avg loss = 0.005078\n",
            "Epoch  23 Time     16.9 lr = 0.140538 avg loss = 0.005060\n",
            "Epoch  23 Time     16.9 lr = 0.140538 avg loss = 0.005061\n",
            "Epoch  23 Time     17.1 lr = 0.140538 avg loss = 0.005072\n",
            "Epoch  23 Time     16.9 lr = 0.140538 avg loss = 0.005060\n",
            "Epoch  23 Time     17.0 lr = 0.140538 avg loss = 0.005065\n",
            "Epoch  23 Time     16.9 lr = 0.140538 avg loss = 0.005074\n",
            "Epoch  23 Time     17.0 lr = 0.140538 avg loss = 0.005076\n",
            "Epoch  23 Time     16.9 lr = 0.140538 avg loss = 0.005093\n",
            "Epoch  23 Time     17.0 lr = 0.140538 avg loss = 0.005097\n",
            "Epoch  23 Time     16.9 lr = 0.140538 avg loss = 0.005091\n",
            "Epoch  23 Time     17.1 lr = 0.140538 avg loss = 0.005096\n",
            "Epoch  23 Time    218.1 lr = 0.140538 avg loss = 0.005096 accuracy = 65.02\n",
            "Epoch  24 Time     31.5 lr = 0.134602 avg loss = 0.004888\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004895\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004905\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004920\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004939\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004947\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004965\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004954\n",
            "Epoch  24 Time     17.1 lr = 0.134602 avg loss = 0.004971\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004981\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.004991\n",
            "Epoch  24 Time     16.9 lr = 0.134602 avg loss = 0.005012\n",
            "Epoch  24 Time    217.6 lr = 0.134602 avg loss = 0.005020 accuracy = 64.40\n",
            "Epoch  25 Time     31.5 lr = 0.128524 avg loss = 0.004763\n",
            "Epoch  25 Time     17.1 lr = 0.128524 avg loss = 0.004791\n",
            "Epoch  25 Time     17.1 lr = 0.128524 avg loss = 0.004866\n",
            "Epoch  25 Time     17.2 lr = 0.128524 avg loss = 0.004873\n",
            "Epoch  25 Time     17.0 lr = 0.128524 avg loss = 0.004896\n",
            "Epoch  25 Time     17.0 lr = 0.128524 avg loss = 0.004892\n",
            "Epoch  25 Time     17.0 lr = 0.128524 avg loss = 0.004891\n",
            "Epoch  25 Time     16.9 lr = 0.128524 avg loss = 0.004899\n",
            "Epoch  25 Time     17.0 lr = 0.128524 avg loss = 0.004912\n",
            "Epoch  25 Time     16.9 lr = 0.128524 avg loss = 0.004927\n",
            "Epoch  25 Time     16.9 lr = 0.128524 avg loss = 0.004929\n",
            "Epoch  25 Time     16.9 lr = 0.128524 avg loss = 0.004933\n",
            "Epoch  25 Time    218.5 lr = 0.128524 avg loss = 0.004936 accuracy = 63.64\n",
            "Epoch  26 Time     31.5 lr = 0.122330 avg loss = 0.004772\n",
            "Epoch  26 Time     17.0 lr = 0.122330 avg loss = 0.004813\n",
            "Epoch  26 Time     16.9 lr = 0.122330 avg loss = 0.004822\n",
            "Epoch  26 Time     17.0 lr = 0.122330 avg loss = 0.004842\n",
            "Epoch  26 Time     17.2 lr = 0.122330 avg loss = 0.004839\n",
            "Epoch  26 Time     17.2 lr = 0.122330 avg loss = 0.004863\n",
            "Epoch  26 Time     17.2 lr = 0.122330 avg loss = 0.004857\n",
            "Epoch  26 Time     17.1 lr = 0.122330 avg loss = 0.004859\n",
            "Epoch  26 Time     16.9 lr = 0.122330 avg loss = 0.004862\n",
            "Epoch  26 Time     17.0 lr = 0.122330 avg loss = 0.004872\n",
            "Epoch  26 Time     16.9 lr = 0.122330 avg loss = 0.004863\n",
            "Epoch  26 Time     17.0 lr = 0.122330 avg loss = 0.004857\n",
            "Epoch  26 Time    219.0 lr = 0.122330 avg loss = 0.004860 accuracy = 66.20\n",
            "Epoch  27 Time     31.7 lr = 0.116044 avg loss = 0.004863\n",
            "Epoch  27 Time     17.0 lr = 0.116044 avg loss = 0.004811\n",
            "Epoch  27 Time     16.8 lr = 0.116044 avg loss = 0.004788\n",
            "Epoch  27 Time     16.8 lr = 0.116044 avg loss = 0.004778\n",
            "Epoch  27 Time     16.8 lr = 0.116044 avg loss = 0.004785\n",
            "Epoch  27 Time     17.0 lr = 0.116044 avg loss = 0.004793\n",
            "Epoch  27 Time     17.1 lr = 0.116044 avg loss = 0.004781\n",
            "Epoch  27 Time     16.9 lr = 0.116044 avg loss = 0.004784\n",
            "Epoch  27 Time     17.1 lr = 0.116044 avg loss = 0.004778\n",
            "Epoch  27 Time     17.1 lr = 0.116044 avg loss = 0.004778\n",
            "Epoch  27 Time     16.9 lr = 0.116044 avg loss = 0.004782\n",
            "Epoch  27 Time     16.9 lr = 0.116044 avg loss = 0.004784\n",
            "Epoch  27 Time    218.3 lr = 0.116044 avg loss = 0.004790 accuracy = 67.10\n",
            "Epoch  28 Time     31.8 lr = 0.109693 avg loss = 0.004517\n",
            "Epoch  28 Time     17.0 lr = 0.109693 avg loss = 0.004555\n",
            "Epoch  28 Time     17.1 lr = 0.109693 avg loss = 0.004589\n",
            "Epoch  28 Time     17.1 lr = 0.109693 avg loss = 0.004583\n",
            "Epoch  28 Time     17.0 lr = 0.109693 avg loss = 0.004605\n",
            "Epoch  28 Time     17.2 lr = 0.109693 avg loss = 0.004640\n",
            "Epoch  28 Time     17.1 lr = 0.109693 avg loss = 0.004637\n",
            "Epoch  28 Time     17.1 lr = 0.109693 avg loss = 0.004662\n",
            "Epoch  28 Time     17.1 lr = 0.109693 avg loss = 0.004665\n",
            "Epoch  28 Time     17.2 lr = 0.109693 avg loss = 0.004669\n",
            "Epoch  28 Time     17.1 lr = 0.109693 avg loss = 0.004676\n",
            "Epoch  28 Time     17.2 lr = 0.109693 avg loss = 0.004679\n",
            "Epoch  28 Time    220.0 lr = 0.109693 avg loss = 0.004681 accuracy = 68.80\n",
            "Epoch  29 Time     32.1 lr = 0.103302 avg loss = 0.004634\n",
            "Epoch  29 Time     17.2 lr = 0.103302 avg loss = 0.004616\n",
            "Epoch  29 Time     17.1 lr = 0.103302 avg loss = 0.004636\n",
            "Epoch  29 Time     17.1 lr = 0.103302 avg loss = 0.004630\n",
            "Epoch  29 Time     17.2 lr = 0.103302 avg loss = 0.004615\n",
            "Epoch  29 Time     16.9 lr = 0.103302 avg loss = 0.004617\n",
            "Epoch  29 Time     17.0 lr = 0.103302 avg loss = 0.004598\n",
            "Epoch  29 Time     17.1 lr = 0.103302 avg loss = 0.004602\n",
            "Epoch  29 Time     17.1 lr = 0.103302 avg loss = 0.004605\n",
            "Epoch  29 Time     17.2 lr = 0.103302 avg loss = 0.004611\n",
            "Epoch  29 Time     17.0 lr = 0.103302 avg loss = 0.004624\n",
            "Epoch  29 Time     17.0 lr = 0.103302 avg loss = 0.004629\n",
            "Epoch  29 Time    219.9 lr = 0.103302 avg loss = 0.004632 accuracy = 67.02\n",
            "Epoch  30 Time     31.9 lr = 0.096898 avg loss = 0.004436\n",
            "Epoch  30 Time     17.0 lr = 0.096898 avg loss = 0.004457\n",
            "Epoch  30 Time     17.0 lr = 0.096898 avg loss = 0.004453\n",
            "Epoch  30 Time     16.9 lr = 0.096898 avg loss = 0.004475\n",
            "Epoch  30 Time     17.0 lr = 0.096898 avg loss = 0.004467\n",
            "Epoch  30 Time     16.9 lr = 0.096898 avg loss = 0.004474\n",
            "Epoch  30 Time     16.8 lr = 0.096898 avg loss = 0.004483\n",
            "Epoch  30 Time     16.8 lr = 0.096898 avg loss = 0.004485\n",
            "Epoch  30 Time     16.9 lr = 0.096898 avg loss = 0.004502\n",
            "Epoch  30 Time     16.9 lr = 0.096898 avg loss = 0.004510\n",
            "Epoch  30 Time     16.8 lr = 0.096898 avg loss = 0.004509\n",
            "Epoch  30 Time     16.9 lr = 0.096898 avg loss = 0.004518\n",
            "Epoch  30 Time    218.2 lr = 0.096898 avg loss = 0.004518 accuracy = 68.78\n",
            "Epoch  31 Time     31.9 lr = 0.090507 avg loss = 0.004241\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004298\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004330\n",
            "Epoch  31 Time     17.0 lr = 0.090507 avg loss = 0.004354\n",
            "Epoch  31 Time     16.8 lr = 0.090507 avg loss = 0.004367\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004381\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004388\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004400\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004420\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004418\n",
            "Epoch  31 Time     16.8 lr = 0.090507 avg loss = 0.004429\n",
            "Epoch  31 Time     16.9 lr = 0.090507 avg loss = 0.004436\n",
            "Epoch  31 Time    217.4 lr = 0.090507 avg loss = 0.004435 accuracy = 67.94\n",
            "Epoch  32 Time     31.6 lr = 0.084156 avg loss = 0.004240\n",
            "Epoch  32 Time     17.0 lr = 0.084156 avg loss = 0.004242\n",
            "Epoch  32 Time     17.0 lr = 0.084156 avg loss = 0.004273\n",
            "Epoch  32 Time     16.8 lr = 0.084156 avg loss = 0.004289\n",
            "Epoch  32 Time     17.0 lr = 0.084156 avg loss = 0.004305\n",
            "Epoch  32 Time     17.0 lr = 0.084156 avg loss = 0.004312\n",
            "Epoch  32 Time     17.0 lr = 0.084156 avg loss = 0.004325\n",
            "Epoch  32 Time     16.9 lr = 0.084156 avg loss = 0.004329\n",
            "Epoch  32 Time     16.9 lr = 0.084156 avg loss = 0.004333\n",
            "Epoch  32 Time     16.9 lr = 0.084156 avg loss = 0.004340\n",
            "Epoch  32 Time     16.9 lr = 0.084156 avg loss = 0.004355\n",
            "Epoch  32 Time     16.9 lr = 0.084156 avg loss = 0.004363\n",
            "Epoch  32 Time    217.8 lr = 0.084156 avg loss = 0.004361 accuracy = 69.32\n",
            "Epoch  33 Time     31.5 lr = 0.077870 avg loss = 0.004243\n",
            "Epoch  33 Time     16.9 lr = 0.077870 avg loss = 0.004202\n",
            "Epoch  33 Time     16.9 lr = 0.077870 avg loss = 0.004229\n",
            "Epoch  33 Time     16.8 lr = 0.077870 avg loss = 0.004246\n",
            "Epoch  33 Time     16.8 lr = 0.077870 avg loss = 0.004240\n",
            "Epoch  33 Time     16.9 lr = 0.077870 avg loss = 0.004243\n",
            "Epoch  33 Time     16.9 lr = 0.077870 avg loss = 0.004244\n",
            "Epoch  33 Time     16.8 lr = 0.077870 avg loss = 0.004262\n",
            "Epoch  33 Time     16.8 lr = 0.077870 avg loss = 0.004263\n",
            "Epoch  33 Time     17.0 lr = 0.077870 avg loss = 0.004271\n",
            "Epoch  33 Time     16.9 lr = 0.077870 avg loss = 0.004280\n",
            "Epoch  33 Time     17.0 lr = 0.077870 avg loss = 0.004288\n",
            "Epoch  33 Time    217.3 lr = 0.077870 avg loss = 0.004288 accuracy = 67.60\n",
            "Epoch  34 Time     31.5 lr = 0.071676 avg loss = 0.004011\n",
            "Epoch  34 Time     16.7 lr = 0.071676 avg loss = 0.004071\n",
            "Epoch  34 Time     16.9 lr = 0.071676 avg loss = 0.004119\n",
            "Epoch  34 Time     16.8 lr = 0.071676 avg loss = 0.004130\n",
            "Epoch  34 Time     16.8 lr = 0.071676 avg loss = 0.004130\n",
            "Epoch  34 Time     16.8 lr = 0.071676 avg loss = 0.004149\n",
            "Epoch  34 Time     16.9 lr = 0.071676 avg loss = 0.004157\n",
            "Epoch  34 Time     16.9 lr = 0.071676 avg loss = 0.004172\n",
            "Epoch  34 Time     16.8 lr = 0.071676 avg loss = 0.004162\n",
            "Epoch  34 Time     16.8 lr = 0.071676 avg loss = 0.004173\n",
            "Epoch  34 Time     16.9 lr = 0.071676 avg loss = 0.004181\n",
            "Epoch  34 Time     16.9 lr = 0.071676 avg loss = 0.004171\n",
            "Epoch  34 Time    216.7 lr = 0.071676 avg loss = 0.004167 accuracy = 69.86\n",
            "Epoch  35 Time     31.4 lr = 0.065598 avg loss = 0.004064\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004052\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004075\n",
            "Epoch  35 Time     17.0 lr = 0.065598 avg loss = 0.004066\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004051\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004068\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004068\n",
            "Epoch  35 Time     16.7 lr = 0.065598 avg loss = 0.004064\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004089\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004089\n",
            "Epoch  35 Time     16.8 lr = 0.065598 avg loss = 0.004099\n",
            "Epoch  35 Time     16.7 lr = 0.065598 avg loss = 0.004098\n",
            "Epoch  35 Time    216.2 lr = 0.065598 avg loss = 0.004102 accuracy = 69.52\n",
            "Epoch  36 Time     31.4 lr = 0.059662 avg loss = 0.003945\n",
            "Epoch  36 Time     16.8 lr = 0.059662 avg loss = 0.003954\n",
            "Epoch  36 Time     16.9 lr = 0.059662 avg loss = 0.003951\n",
            "Epoch  36 Time     16.9 lr = 0.059662 avg loss = 0.003968\n",
            "Epoch  36 Time     16.9 lr = 0.059662 avg loss = 0.003979\n",
            "Epoch  36 Time     16.8 lr = 0.059662 avg loss = 0.003975\n",
            "Epoch  36 Time     16.9 lr = 0.059662 avg loss = 0.003980\n",
            "Epoch  36 Time     17.0 lr = 0.059662 avg loss = 0.003985\n",
            "Epoch  36 Time     16.9 lr = 0.059662 avg loss = 0.003993\n",
            "Epoch  36 Time     17.0 lr = 0.059662 avg loss = 0.003993\n",
            "Epoch  36 Time     16.8 lr = 0.059662 avg loss = 0.004005\n",
            "Epoch  36 Time     16.9 lr = 0.059662 avg loss = 0.004011\n",
            "Epoch  36 Time    217.2 lr = 0.059662 avg loss = 0.004010 accuracy = 69.00\n",
            "Epoch  37 Time     31.5 lr = 0.053892 avg loss = 0.003830\n",
            "Epoch  37 Time     16.8 lr = 0.053892 avg loss = 0.003951\n",
            "Epoch  37 Time     17.0 lr = 0.053892 avg loss = 0.003942\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003925\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003932\n",
            "Epoch  37 Time     16.8 lr = 0.053892 avg loss = 0.003941\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003946\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003938\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003927\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003939\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003947\n",
            "Epoch  37 Time     16.9 lr = 0.053892 avg loss = 0.003947\n",
            "Epoch  37 Time    217.3 lr = 0.053892 avg loss = 0.003953 accuracy = 70.08\n",
            "Epoch  38 Time     31.7 lr = 0.048313 avg loss = 0.003778\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003816\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003808\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003804\n",
            "Epoch  38 Time     16.7 lr = 0.048313 avg loss = 0.003826\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003832\n",
            "Epoch  38 Time     16.8 lr = 0.048313 avg loss = 0.003834\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003832\n",
            "Epoch  38 Time     16.8 lr = 0.048313 avg loss = 0.003839\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003842\n",
            "Epoch  38 Time     16.8 lr = 0.048313 avg loss = 0.003835\n",
            "Epoch  38 Time     16.9 lr = 0.048313 avg loss = 0.003835\n",
            "Epoch  38 Time    216.9 lr = 0.048313 avg loss = 0.003844 accuracy = 70.52\n",
            "Epoch  39 Time     31.5 lr = 0.042946 avg loss = 0.003744\n",
            "Epoch  39 Time     16.8 lr = 0.042946 avg loss = 0.003788\n",
            "Epoch  39 Time     16.7 lr = 0.042946 avg loss = 0.003757\n",
            "Epoch  39 Time     16.8 lr = 0.042946 avg loss = 0.003769\n",
            "Epoch  39 Time     16.8 lr = 0.042946 avg loss = 0.003748\n",
            "Epoch  39 Time     16.8 lr = 0.042946 avg loss = 0.003741\n",
            "Epoch  39 Time     16.7 lr = 0.042946 avg loss = 0.003751\n",
            "Epoch  39 Time     16.8 lr = 0.042946 avg loss = 0.003754\n",
            "Epoch  39 Time     16.8 lr = 0.042946 avg loss = 0.003763\n",
            "Epoch  39 Time     16.6 lr = 0.042946 avg loss = 0.003774\n",
            "Epoch  39 Time     16.9 lr = 0.042946 avg loss = 0.003773\n",
            "Epoch  39 Time     16.7 lr = 0.042946 avg loss = 0.003773\n",
            "Epoch  39 Time    215.8 lr = 0.042946 avg loss = 0.003776 accuracy = 71.46\n",
            "Epoch  40 Time     31.2 lr = 0.037813 avg loss = 0.003643\n",
            "Epoch  40 Time     16.8 lr = 0.037813 avg loss = 0.003662\n",
            "Epoch  40 Time     16.8 lr = 0.037813 avg loss = 0.003659\n",
            "Epoch  40 Time     16.9 lr = 0.037813 avg loss = 0.003658\n",
            "Epoch  40 Time     16.7 lr = 0.037813 avg loss = 0.003672\n",
            "Epoch  40 Time     16.8 lr = 0.037813 avg loss = 0.003660\n",
            "Epoch  40 Time     16.7 lr = 0.037813 avg loss = 0.003668\n",
            "Epoch  40 Time     16.9 lr = 0.037813 avg loss = 0.003670\n",
            "Epoch  40 Time     16.8 lr = 0.037813 avg loss = 0.003675\n",
            "Epoch  40 Time     17.0 lr = 0.037813 avg loss = 0.003680\n",
            "Epoch  40 Time     16.9 lr = 0.037813 avg loss = 0.003681\n",
            "Epoch  40 Time     16.9 lr = 0.037813 avg loss = 0.003682\n",
            "Epoch  40 Time    216.4 lr = 0.037813 avg loss = 0.003676 accuracy = 71.46\n",
            "Epoch  41 Time     31.8 lr = 0.032937 avg loss = 0.003581\n",
            "Epoch  41 Time     16.8 lr = 0.032937 avg loss = 0.003551\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003547\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003543\n",
            "Epoch  41 Time     17.0 lr = 0.032937 avg loss = 0.003554\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003557\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003574\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003581\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003579\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003579\n",
            "Epoch  41 Time     16.9 lr = 0.032937 avg loss = 0.003581\n",
            "Epoch  41 Time     16.8 lr = 0.032937 avg loss = 0.003583\n",
            "Epoch  41 Time    217.4 lr = 0.032937 avg loss = 0.003587 accuracy = 70.64\n",
            "Epoch  42 Time     31.3 lr = 0.028337 avg loss = 0.003566\n",
            "Epoch  42 Time     16.8 lr = 0.028337 avg loss = 0.003548\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003525\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003536\n",
            "Epoch  42 Time     16.8 lr = 0.028337 avg loss = 0.003544\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003525\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003514\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003521\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003516\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003512\n",
            "Epoch  42 Time     16.9 lr = 0.028337 avg loss = 0.003513\n",
            "Epoch  42 Time     17.0 lr = 0.028337 avg loss = 0.003517\n",
            "Epoch  42 Time    217.1 lr = 0.028337 avg loss = 0.003519 accuracy = 70.52\n",
            "Epoch  43 Time     31.7 lr = 0.024032 avg loss = 0.003361\n",
            "Epoch  43 Time     16.8 lr = 0.024032 avg loss = 0.003371\n",
            "Epoch  43 Time     16.9 lr = 0.024032 avg loss = 0.003383\n",
            "Epoch  43 Time     16.9 lr = 0.024032 avg loss = 0.003417\n",
            "Epoch  43 Time     16.8 lr = 0.024032 avg loss = 0.003413\n",
            "Epoch  43 Time     16.9 lr = 0.024032 avg loss = 0.003418\n",
            "Epoch  43 Time     16.9 lr = 0.024032 avg loss = 0.003409\n",
            "Epoch  43 Time     16.9 lr = 0.024032 avg loss = 0.003411\n",
            "Epoch  43 Time     16.8 lr = 0.024032 avg loss = 0.003424\n",
            "Epoch  43 Time     16.8 lr = 0.024032 avg loss = 0.003430\n",
            "Epoch  43 Time     16.8 lr = 0.024032 avg loss = 0.003430\n",
            "Epoch  43 Time     16.9 lr = 0.024032 avg loss = 0.003433\n",
            "Epoch  43 Time    217.2 lr = 0.024032 avg loss = 0.003436 accuracy = 72.54\n",
            "Epoch  44 Time     31.5 lr = 0.020039 avg loss = 0.003285\n",
            "Epoch  44 Time     16.9 lr = 0.020039 avg loss = 0.003283\n",
            "Epoch  44 Time     16.8 lr = 0.020039 avg loss = 0.003266\n",
            "Epoch  44 Time     16.8 lr = 0.020039 avg loss = 0.003295\n",
            "Epoch  44 Time     16.9 lr = 0.020039 avg loss = 0.003296\n",
            "Epoch  44 Time     17.0 lr = 0.020039 avg loss = 0.003299\n",
            "Epoch  44 Time     17.0 lr = 0.020039 avg loss = 0.003307\n",
            "Epoch  44 Time     16.9 lr = 0.020039 avg loss = 0.003303\n",
            "Epoch  44 Time     16.9 lr = 0.020039 avg loss = 0.003310\n",
            "Epoch  44 Time     16.8 lr = 0.020039 avg loss = 0.003310\n",
            "Epoch  44 Time     16.9 lr = 0.020039 avg loss = 0.003319\n",
            "Epoch  44 Time     16.8 lr = 0.020039 avg loss = 0.003324\n",
            "Epoch  44 Time    217.2 lr = 0.020039 avg loss = 0.003326 accuracy = 71.66\n",
            "Epoch  45 Time     31.5 lr = 0.016375 avg loss = 0.003148\n",
            "Epoch  45 Time     16.9 lr = 0.016375 avg loss = 0.003233\n",
            "Epoch  45 Time     16.9 lr = 0.016375 avg loss = 0.003227\n",
            "Epoch  45 Time     17.0 lr = 0.016375 avg loss = 0.003222\n",
            "Epoch  45 Time     16.9 lr = 0.016375 avg loss = 0.003216\n",
            "Epoch  45 Time     16.9 lr = 0.016375 avg loss = 0.003234\n",
            "Epoch  45 Time     16.8 lr = 0.016375 avg loss = 0.003243\n",
            "Epoch  45 Time     16.9 lr = 0.016375 avg loss = 0.003251\n",
            "Epoch  45 Time     16.8 lr = 0.016375 avg loss = 0.003257\n",
            "Epoch  45 Time     16.8 lr = 0.016375 avg loss = 0.003260\n",
            "Epoch  45 Time     16.8 lr = 0.016375 avg loss = 0.003262\n",
            "Epoch  45 Time     16.8 lr = 0.016375 avg loss = 0.003263\n",
            "Epoch  45 Time    216.8 lr = 0.016375 avg loss = 0.003266 accuracy = 72.78\n",
            "Epoch  46 Time     31.3 lr = 0.013055 avg loss = 0.003209\n",
            "Epoch  46 Time     16.7 lr = 0.013055 avg loss = 0.003194\n",
            "Epoch  46 Time     16.8 lr = 0.013055 avg loss = 0.003166\n",
            "Epoch  46 Time     16.8 lr = 0.013055 avg loss = 0.003157\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003167\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003175\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003195\n",
            "Epoch  46 Time     16.8 lr = 0.013055 avg loss = 0.003212\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003224\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003225\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003221\n",
            "Epoch  46 Time     16.9 lr = 0.013055 avg loss = 0.003224\n",
            "Epoch  46 Time    216.8 lr = 0.013055 avg loss = 0.003224 accuracy = 72.98\n",
            "Epoch  47 Time     31.6 lr = 0.010093 avg loss = 0.003061\n",
            "Epoch  47 Time     17.0 lr = 0.010093 avg loss = 0.003073\n",
            "Epoch  47 Time     17.5 lr = 0.010093 avg loss = 0.003120\n",
            "Epoch  47 Time     17.3 lr = 0.010093 avg loss = 0.003136\n",
            "Epoch  47 Time     17.2 lr = 0.010093 avg loss = 0.003129\n",
            "Epoch  47 Time     17.1 lr = 0.010093 avg loss = 0.003120\n",
            "Epoch  47 Time     17.0 lr = 0.010093 avg loss = 0.003106\n",
            "Epoch  47 Time     17.0 lr = 0.010093 avg loss = 0.003109\n",
            "Epoch  47 Time     17.0 lr = 0.010093 avg loss = 0.003117\n",
            "Epoch  47 Time     17.0 lr = 0.010093 avg loss = 0.003113\n",
            "Epoch  47 Time     17.0 lr = 0.010093 avg loss = 0.003113\n",
            "Epoch  47 Time     16.9 lr = 0.010093 avg loss = 0.003120\n",
            "Epoch  47 Time    219.8 lr = 0.010093 avg loss = 0.003118 accuracy = 73.34\n",
            "Epoch  48 Time     31.6 lr = 0.007501 avg loss = 0.003069\n",
            "Epoch  48 Time     16.9 lr = 0.007501 avg loss = 0.003047\n",
            "Epoch  48 Time     17.0 lr = 0.007501 avg loss = 0.003053\n",
            "Epoch  48 Time     16.9 lr = 0.007501 avg loss = 0.003060\n",
            "Epoch  48 Time     17.0 lr = 0.007501 avg loss = 0.003076\n",
            "Epoch  48 Time     16.9 lr = 0.007501 avg loss = 0.003078\n",
            "Epoch  48 Time     17.0 lr = 0.007501 avg loss = 0.003085\n",
            "Epoch  48 Time     17.0 lr = 0.007501 avg loss = 0.003080\n",
            "Epoch  48 Time     17.0 lr = 0.007501 avg loss = 0.003076\n",
            "Epoch  48 Time     16.9 lr = 0.007501 avg loss = 0.003084\n",
            "Epoch  48 Time     16.9 lr = 0.007501 avg loss = 0.003082\n",
            "Epoch  48 Time     16.9 lr = 0.007501 avg loss = 0.003089\n",
            "Epoch  48 Time    218.0 lr = 0.007501 avg loss = 0.003090 accuracy = 73.24\n",
            "Epoch  49 Time     31.5 lr = 0.005289 avg loss = 0.003033\n",
            "Epoch  49 Time     17.0 lr = 0.005289 avg loss = 0.003098\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003080\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003075\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003090\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003099\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003089\n",
            "Epoch  49 Time     17.0 lr = 0.005289 avg loss = 0.003078\n",
            "Epoch  49 Time     17.0 lr = 0.005289 avg loss = 0.003082\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003083\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003079\n",
            "Epoch  49 Time     16.9 lr = 0.005289 avg loss = 0.003070\n",
            "Epoch  49 Time    217.6 lr = 0.005289 avg loss = 0.003071 accuracy = 73.48\n",
            "Epoch  50 Time     31.7 lr = 0.003467 avg loss = 0.003025\n",
            "Epoch  50 Time     16.9 lr = 0.003467 avg loss = 0.003006\n",
            "Epoch  50 Time     16.9 lr = 0.003467 avg loss = 0.003016\n",
            "Epoch  50 Time     17.0 lr = 0.003467 avg loss = 0.003016\n",
            "Epoch  50 Time     16.9 lr = 0.003467 avg loss = 0.003004\n",
            "Epoch  50 Time     16.9 lr = 0.003467 avg loss = 0.003005\n",
            "Epoch  50 Time     16.9 lr = 0.003467 avg loss = 0.003001\n",
            "Epoch  50 Time     16.9 lr = 0.003467 avg loss = 0.003010\n",
            "Epoch  50 Time     16.8 lr = 0.003467 avg loss = 0.003001\n",
            "Epoch  50 Time     17.0 lr = 0.003467 avg loss = 0.003003\n",
            "Epoch  50 Time     17.0 lr = 0.003467 avg loss = 0.003009\n",
            "Epoch  50 Time     17.0 lr = 0.003467 avg loss = 0.003017\n",
            "Epoch  50 Time    217.9 lr = 0.003467 avg loss = 0.003021 accuracy = 73.44\n",
            "Epoch  51 Time     31.6 lr = 0.002042 avg loss = 0.002857\n",
            "Epoch  51 Time     16.9 lr = 0.002042 avg loss = 0.002957\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002974\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002965\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002969\n",
            "Epoch  51 Time     17.1 lr = 0.002042 avg loss = 0.002975\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002981\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002985\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002980\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002983\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002992\n",
            "Epoch  51 Time     17.0 lr = 0.002042 avg loss = 0.002992\n",
            "Epoch  51 Time    218.6 lr = 0.002042 avg loss = 0.002991 accuracy = 73.52\n",
            "Epoch  52 Time     31.6 lr = 0.001020 avg loss = 0.002919\n",
            "Epoch  52 Time     17.1 lr = 0.001020 avg loss = 0.002914\n",
            "Epoch  52 Time     16.9 lr = 0.001020 avg loss = 0.002936\n",
            "Epoch  52 Time     17.1 lr = 0.001020 avg loss = 0.002950\n",
            "Epoch  52 Time     17.0 lr = 0.001020 avg loss = 0.002946\n",
            "Epoch  52 Time     17.1 lr = 0.001020 avg loss = 0.002955\n",
            "Epoch  52 Time     17.0 lr = 0.001020 avg loss = 0.002952\n",
            "Epoch  52 Time     17.0 lr = 0.001020 avg loss = 0.002958\n",
            "Epoch  52 Time     17.0 lr = 0.001020 avg loss = 0.002961\n",
            "Epoch  52 Time     17.0 lr = 0.001020 avg loss = 0.002962\n",
            "Epoch  52 Time     17.1 lr = 0.001020 avg loss = 0.002955\n",
            "Epoch  52 Time     17.0 lr = 0.001020 avg loss = 0.002957\n",
            "Epoch  52 Time    219.0 lr = 0.001020 avg loss = 0.002964 accuracy = 73.28\n",
            "Epoch  53 Time     31.8 lr = 0.000405 avg loss = 0.002857\n",
            "Epoch  53 Time     17.1 lr = 0.000405 avg loss = 0.002844\n",
            "Epoch  53 Time     17.3 lr = 0.000405 avg loss = 0.002875\n",
            "Epoch  53 Time     17.1 lr = 0.000405 avg loss = 0.002905\n",
            "Epoch  53 Time     16.9 lr = 0.000405 avg loss = 0.002923\n",
            "Epoch  53 Time     17.1 lr = 0.000405 avg loss = 0.002930\n",
            "Epoch  53 Time     17.0 lr = 0.000405 avg loss = 0.002928\n",
            "Epoch  53 Time     17.1 lr = 0.000405 avg loss = 0.002943\n",
            "Epoch  53 Time     17.0 lr = 0.000405 avg loss = 0.002947\n",
            "Epoch  53 Time     17.0 lr = 0.000405 avg loss = 0.002941\n",
            "Epoch  53 Time     17.0 lr = 0.000405 avg loss = 0.002944\n",
            "Epoch  53 Time     17.1 lr = 0.000405 avg loss = 0.002949\n",
            "Epoch  53 Time    219.5 lr = 0.000405 avg loss = 0.002954 accuracy = 73.50\n",
            "Epoch  54 Time     31.9 lr = 0.000200 avg loss = 0.002868\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002945\n",
            "Epoch  54 Time     17.0 lr = 0.000200 avg loss = 0.002973\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002951\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002938\n",
            "Epoch  54 Time     16.8 lr = 0.000200 avg loss = 0.002942\n",
            "Epoch  54 Time     17.0 lr = 0.000200 avg loss = 0.002946\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002951\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002956\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002965\n",
            "Epoch  54 Time     17.0 lr = 0.000200 avg loss = 0.002963\n",
            "Epoch  54 Time     16.9 lr = 0.000200 avg loss = 0.002961\n",
            "Epoch  54 Time    217.9 lr = 0.000200 avg loss = 0.002959 accuracy = 73.44\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}