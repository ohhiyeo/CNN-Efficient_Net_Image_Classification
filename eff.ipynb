{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eff.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4k_pjJKcER7"
      },
      "source": [
        "#####################################################################################\n",
        "#                                                                                   #\n",
        "#       Standard:                                                                   #\n",
        "#       Epoch   0 Time    342.0 lr = 0.002000 avg loss = 0.016711 accuracy = 10.38  #\n",
        "#       Epoch   1 Time    339.3 lr = 0.041600 avg loss = 0.012989 accuracy = 29.54  #\n",
        "#       Epoch   2 Time    339.0 lr = 0.081200 avg loss = 0.010873 accuracy = 35.42  #\n",
        "#       Epoch   3 Time    338.6 lr = 0.120800 avg loss = 0.009820 accuracy = 41.32  #\n",
        "#       Epoch   4 Time    339.1 lr = 0.160400 avg loss = 0.009088 accuracy = 44.38  #\n",
        "#       Epoch   5 Time    338.3 lr = 0.200000 avg loss = 0.008535 accuracy = 46.74  #\n",
        "#       Epoch   6 Time    338.3 lr = 0.199795 avg loss = 0.007995 accuracy = 47.94  #\n",
        "#       Epoch   7 Time    339.8 lr = 0.199180 avg loss = 0.007604 accuracy = 49.38  #\n",
        "#       Epoch   8 Time    340.0 lr = 0.198158 avg loss = 0.007319 accuracy = 54.20  #\n",
        "#       Epoch   9 Time    339.1 lr = 0.196733 avg loss = 0.007079 accuracy = 54.58  #\n",
        "#       Epoch  10 Time    338.8 lr = 0.194911 avg loss = 0.006851 accuracy = 57.20  #\n",
        "#       Epoch  11 Time    338.3 lr = 0.192699 avg loss = 0.006709 accuracy = 57.54  #\n",
        "#       Epoch  12 Time    338.3 lr = 0.190107 avg loss = 0.006525 accuracy = 58.52  #\n",
        "#       Epoch  13 Time    338.7 lr = 0.187145 avg loss = 0.006418 accuracy = 59.00  #\n",
        "#       Epoch  14 Time    338.3 lr = 0.183825 avg loss = 0.006292 accuracy = 57.68  #\n",
        "#       Epoch  15 Time    338.2 lr = 0.180161 avg loss = 0.006183 accuracy = 60.62  #\n",
        "#       Epoch  16 Time    338.4 lr = 0.176168 avg loss = 0.006044 accuracy = 58.66  #\n",
        "#       Epoch  17 Time    338.8 lr = 0.171863 avg loss = 0.006002 accuracy = 57.46  #\n",
        "#       Epoch  18 Time    338.7 lr = 0.167263 avg loss = 0.005905 accuracy = 61.96  #\n",
        "#       Epoch  19 Time    338.8 lr = 0.162387 avg loss = 0.005786 accuracy = 63.76  #\n",
        "#       Epoch  20 Time    338.3 lr = 0.157254 avg loss = 0.005743 accuracy = 63.50  #\n",
        "#       Epoch  21 Time    338.5 lr = 0.151887 avg loss = 0.005629 accuracy = 65.34  #\n",
        "#       Epoch  22 Time    338.0 lr = 0.146308 avg loss = 0.005604 accuracy = 62.34  #\n",
        "#       Epoch  23 Time    338.5 lr = 0.140538 avg loss = 0.005490 accuracy = 65.64  #\n",
        "#       Epoch  24 Time    338.5 lr = 0.134602 avg loss = 0.005404 accuracy = 65.64  #\n",
        "#       Epoch  25 Time    338.4 lr = 0.128524 avg loss = 0.005331 accuracy = 65.20  #\n",
        "#       Epoch  26 Time    338.2 lr = 0.122330 avg loss = 0.005273 accuracy = 65.42  #\n",
        "#       Epoch  27 Time    338.7 lr = 0.116044 avg loss = 0.005185 accuracy = 67.48  #\n",
        "#       Epoch  28 Time    339.0 lr = 0.109693 avg loss = 0.005093 accuracy = 65.72  #\n",
        "#       Epoch  29 Time    339.0 lr = 0.103302 avg loss = 0.005028 accuracy = 63.58  #\n",
        "#       Epoch  30 Time    338.7 lr = 0.096898 avg loss = 0.004962 accuracy = 65.06  #\n",
        "#       Epoch  31 Time    338.4 lr = 0.090507 avg loss = 0.004843 accuracy = 65.68  #\n",
        "#       Epoch  32 Time    337.6 lr = 0.084156 avg loss = 0.004794 accuracy = 67.80  #\n",
        "#       Epoch  33 Time    337.6 lr = 0.077870 avg loss = 0.004692 accuracy = 67.72  #\n",
        "#       Epoch  34 Time    337.4 lr = 0.071676 avg loss = 0.004609 accuracy = 67.96  #\n",
        "#       Epoch  35 Time    337.4 lr = 0.065598 avg loss = 0.004525 accuracy = 67.64  #\n",
        "#       Epoch  36 Time    338.7 lr = 0.059662 avg loss = 0.004437 accuracy = 67.90  #\n",
        "#       Epoch  37 Time    337.6 lr = 0.053892 avg loss = 0.004353 accuracy = 70.16  #\n",
        "#       Epoch  38 Time    337.3 lr = 0.048313 avg loss = 0.004280 accuracy = 69.38  #\n",
        "#       Epoch  39 Time    337.2 lr = 0.042946 avg loss = 0.004178 accuracy = 69.92  #\n",
        "#       Epoch  40 Time    337.3 lr = 0.037813 avg loss = 0.004097 accuracy = 69.54  #\n",
        "#       Epoch  41 Time    337.3 lr = 0.032937 avg loss = 0.004007 accuracy = 70.76  #\n",
        "#       Epoch  42 Time    337.4 lr = 0.028337 avg loss = 0.003928 accuracy = 70.32  #\n",
        "#       Epoch  43 Time    337.7 lr = 0.024032 avg loss = 0.003853 accuracy = 70.56  #\n",
        "#       Epoch  44 Time    337.6 lr = 0.020039 avg loss = 0.003755 accuracy = 70.44  #\n",
        "#       Epoch  45 Time    337.5 lr = 0.016375 avg loss = 0.003695 accuracy = 71.72  #\n",
        "#       Epoch  46 Time    338.0 lr = 0.013055 avg loss = 0.003603 accuracy = 71.56  #\n",
        "#       Epoch  47 Time    339.2 lr = 0.010093 avg loss = 0.003547 accuracy = 71.06  #\n",
        "#       Epoch  48 Time    338.7 lr = 0.007501 avg loss = 0.003501 accuracy = 72.06  #\n",
        "#       Epoch  49 Time    338.4 lr = 0.005289 avg loss = 0.003451 accuracy = 71.36  #\n",
        "#       Epoch  50 Time    339.8 lr = 0.003467 avg loss = 0.003432 accuracy = 71.88  #\n",
        "#       Epoch  51 Time    339.0 lr = 0.002042 avg loss = 0.003396 accuracy = 71.74  #\n",
        "#       Epoch  52 Time    338.9 lr = 0.001020 avg loss = 0.003373 accuracy = 71.90  #\n",
        "#       Epoch  53 Time    338.6 lr = 0.000405 avg loss = 0.003371 accuracy = 71.94  #\n",
        "#       Epoch  54 Time    338.6 lr = 0.000200 avg loss = 0.003368 accuracy = 72.08  #\n",
        "#                                                                                   #\n",
        "#                                                                                   #\n",
        "#####################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNujOXllc7gf"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# IMPORT                                                                        #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# torch\n",
        "import torch\n",
        "import torch.nn       as     nn\n",
        "import torch.optim    as     optim\n",
        "from   torch.autograd import Function\n",
        "\n",
        "# torch utils\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# additional libraries\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import time\n",
        "import math\n",
        "import numpy             as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpAXQHuzdEAj"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# PARAMETERS                                                                    #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# data\n",
        "DATA_DIR_1        = 'data'\n",
        "DATA_DIR_2        = 'data/imagenet64'\n",
        "DATA_DIR_TRAIN    = 'data/imagenet64/train'\n",
        "DATA_DIR_TEST     = 'data/imagenet64/val'\n",
        "DATA_FILE_TRAIN_1 = 'Train1.zip'\n",
        "DATA_FILE_TRAIN_2 = 'Train2.zip'\n",
        "DATA_FILE_TRAIN_3 = 'Train3.zip'\n",
        "DATA_FILE_TRAIN_4 = 'Train4.zip'\n",
        "DATA_FILE_TRAIN_5 = 'Train5.zip'\n",
        "DATA_FILE_TEST_1  = 'Val1.zip'\n",
        "DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'\n",
        "DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'\n",
        "DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'\n",
        "DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'\n",
        "DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'\n",
        "DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'\n",
        "DATA_BATCH_SIZE   = 256\n",
        "DATA_NUM_WORKERS  = 4\n",
        "DATA_NUM_CHANNELS = 3\n",
        "DATA_NUM_CLASSES  = 100\n",
        "DATA_RESIZE       = 64\n",
        "DATA_CROP         = 56\n",
        "DATA_MEAN         = (0.485, 0.456, 0.406)\n",
        "DATA_STD_DEV      = (0.229, 0.224, 0.225)\n",
        "\n",
        "# model\n",
        "MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation\n",
        "MODEL_LEVEL_2_BLOCKS   = 1\n",
        "MODEL_LEVEL_3_BLOCKS   = 2\n",
        "MODEL_LEVEL_4_BLOCKS   = 3\n",
        "MODEL_LEVEL_5_BLOCKS   = 4\n",
        "MODEL_LEVEL_1_CHANNELS = 16\n",
        "MODEL_LEVEL_2_CHANNELS = 24\n",
        "MODEL_LEVEL_3_CHANNELS = 40\n",
        "MODEL_LEVEL_4_CHANNELS = 80\n",
        "MODEL_LEVEL_5_CHANNELS = 160\n",
        "MODEL_HEAD_CONV_IN_CHANNELS = 320\n",
        "MODEL_HEAD_CONV_OUT_CHANNELS = 1280\n",
        "\n",
        "# training\n",
        "TRAIN_LR_MAX              = 0.2\n",
        "TRAIN_LR_INIT_SCALE       = 0.01\n",
        "TRAIN_LR_FINAL_SCALE      = 0.001\n",
        "TRAIN_LR_INIT_EPOCHS      = 5\n",
        "TRAIN_LR_FINAL_EPOCHS     = 50 # 100\n",
        "TRAIN_NUM_EPOCHS          = TRAIN_LR_INIT_EPOCHS + TRAIN_LR_FINAL_EPOCHS\n",
        "TRAIN_LR_INIT             = TRAIN_LR_MAX*TRAIN_LR_INIT_SCALE\n",
        "TRAIN_LR_FINAL            = TRAIN_LR_MAX*TRAIN_LR_FINAL_SCALE\n",
        "TRAIN_INTRA_EPOCH_DISPLAY = 10000\n",
        "\n",
        "# file\n",
        "FILE_NAME_CHECK      = 'EffNetStyleCheck.pt'\n",
        "FILE_NAME_BEST       = 'EffNetStyleBest.pt'\n",
        "FILE_SAVE            = True\n",
        "FILE_LOAD            = False\n",
        "FILE_EXTEND_TRAINING = False\n",
        "FILE_NEW_OPTIMIZER   = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yr4253qTdJx9"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# DATA                                                                          #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# create a local directory structure for data storage\n",
        "if (os.path.exists(DATA_DIR_1) == False):\n",
        "    os.mkdir(DATA_DIR_1)\n",
        "if (os.path.exists(DATA_DIR_2) == False):\n",
        "    os.mkdir(DATA_DIR_2)\n",
        "if (os.path.exists(DATA_DIR_TRAIN) == False):\n",
        "    os.mkdir(DATA_DIR_TRAIN)\n",
        "if (os.path.exists(DATA_DIR_TEST) == False):\n",
        "    os.mkdir(DATA_DIR_TEST)\n",
        "\n",
        "# download data\n",
        "if (os.path.exists(DATA_FILE_TRAIN_1) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_2) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_3) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_4) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)\n",
        "if (os.path.exists(DATA_FILE_TRAIN_5) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)\n",
        "if (os.path.exists(DATA_FILE_TEST_1) == False):\n",
        "    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)\n",
        "\n",
        "# extract data\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TRAIN)\n",
        "with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR_TEST)\n",
        "\n",
        "# transforms\n",
        "transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
        "transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])\n",
        "\n",
        "# data sets\n",
        "dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)\n",
        "dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)\n",
        "\n",
        "# data loader\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True)\n",
        "dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpFqdJ4VdSI-"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# NETWORK BUILDING BLOCK                                                        #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# inverted residual block\n",
        "class InvResBlock(nn.Module):\n",
        "\n",
        "    # initialization\n",
        "    # Q: do we need Group Size?\n",
        "    def __init__(self, Ni, Ne, No, F, S):\n",
        "\n",
        "        # parent initialization\n",
        "        # create all of the operators for the inverted residual block in fig 2a\n",
        "        # of the paper; note that parameter names were chosen to match the paper\n",
        "        super(InvResBlock, self).__init__()\n",
        "\n",
        "        if ((Ni==No) and S==1):\n",
        "          self.id = True\n",
        "          # self.conv0 = nn.Conv2d(Ni, Ne, kernel_size=1, stride=S, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
        "          # self.bn = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        else:\n",
        "          self.id = False\n",
        "        \n",
        "        P = np.floor(F/2).astype(int)\n",
        "        self.conv1 = nn.Conv2d(Ni, Ne, kernel_size=1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
        "        self.bn1   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(Ne, Ne, kernel_size=F, stride=S, padding=P, dilation=1, groups=Ne, bias=False, padding_mode='zeros')\n",
        "        self.bn2   = nn.BatchNorm2d(Ne, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv3 = nn.Conv2d(Ne, No, kernel_size=1,stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
        "        self.bn3   = nn.BatchNorm2d(No, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "\n",
        "        # sum\n",
        "        # self.relu0 = nn.ReLU()\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    # forward path\n",
        "    def forward(self, x):\n",
        "\n",
        "        # map input x to output y for the inverted residual block in fig 2a of\n",
        "        # the paper via connecting the operators defined in the initialization\n",
        "        # and return output y\n",
        "        if (self.id == True):\n",
        "          id = x\n",
        "\n",
        "        res = self.conv1(x)\n",
        "        res = self.bn1(res)\n",
        "        res = self.relu1(res)\n",
        "        res = self.conv2(res)\n",
        "        res = self.bn2(res)\n",
        "        res = self.relu2(res)\n",
        "        res = self.conv3(res)\n",
        "        res = self.bn3(res)\n",
        "\n",
        "        y = res\n",
        "\n",
        "        if (self.id == True):\n",
        "          y = y+id\n",
        "        # y = self.relu0(y)\n",
        "\n",
        "        # return\n",
        "        return y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD0RsAHbdZ65",
        "outputId": "405bebe1-7e0d-4bca-9191-6cc7164690ac"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# NETWORK                                                                       #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# define\n",
        "class Model(nn.Module):\n",
        "\n",
        "    # initialization\n",
        "    # add necessary parameters to the init function to create the model defined\n",
        "    # in table 1 of the paper\n",
        "    def __init__(self, data_num_channels,\n",
        "                 model_level_1_blocks, model_level_1_channels,\n",
        "                 model_level_2_blocks, model_level_2_channels,\n",
        "                 model_level_3_blocks, model_level_3_channels,\n",
        "                 model_level_4_blocks, model_level_4_channels,\n",
        "                 model_level_5_blocks, model_level_5_channels,\n",
        "                 model_head_conv_in_channels, model_head_conv_out_channels, data_num_classes): \n",
        "\n",
        "        # parent initialization\n",
        "        super(Model, self).__init__()\n",
        "        # create all of the operators for the network defined in table 1 of the\n",
        "        # paper using a combination of Python, standard PyTorch operators and\n",
        "        # the previously defined InvResBlock class\n",
        "        stride1 = 1\n",
        "        stride2 = 2\n",
        "        stride3 = 2\n",
        "        stride4 = 2\n",
        "        stride5 = 1\n",
        "\n",
        "\n",
        "        # MODEL_LEVEL_1_BLOCKS   = 1 # used but ignored in model creation\n",
        "        # MODEL_LEVEL_2_BLOCKS   = 1\n",
        "        # MODEL_LEVEL_3_BLOCKS   = 2\n",
        "        # MODEL_LEVEL_4_BLOCKS   = 3\n",
        "        # MODEL_LEVEL_5_BLOCKS   = 4\n",
        "        # MODEL_LEVEL_1_CHANNELS = 16\n",
        "        # MODEL_LEVEL_2_CHANNELS = 24\n",
        "        # MODEL_LEVEL_3_CHANNELS = 40\n",
        "        # MODEL_LEVEL_4_CHANNELS = 80\n",
        "        # MODEL_LEVEL_5_CHANNELS = 160\n",
        "\n",
        "        #Tail\n",
        "        self.tail = nn.ModuleList()\n",
        "        self.tail.append(nn.Conv2d(data_num_channels, model_level_1_channels, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros'))\n",
        "        self.tail.append(nn.BatchNorm2d(model_level_1_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
        "        self.tail.append(nn.ReLU())\n",
        "\n",
        "        # encoder level 1-Body\n",
        "        self.enc_1 = nn.ModuleList()\n",
        "        for n in range(model_level_1_blocks - 1):\n",
        "            self.enc_1.append(InvResBlock(model_level_1_channels, 4*model_level_1_channels, model_level_1_channels, 3, 1))\n",
        "        self.enc_1.append(InvResBlock(model_level_1_channels, 4*model_level_1_channels, model_level_2_channels, 3, stride1))\n",
        "\n",
        "        # encoder level 2-Body\n",
        "        self.enc_2 = nn.ModuleList()\n",
        "        for n in range(model_level_2_blocks - 1):\n",
        "            self.enc_2.append(InvResBlock(model_level_2_channels, 4*model_level_2_channels, model_level_2_channels, 3, 1))\n",
        "        self.enc_2.append(InvResBlock(model_level_2_channels, 4*model_level_2_channels, model_level_3_channels, 3, stride2))\n",
        "\n",
        "        # encoder level 3-Body\n",
        "        self.enc_3 = nn.ModuleList()\n",
        "        for n in range(model_level_3_blocks - 1):\n",
        "            self.enc_3.append(InvResBlock(model_level_3_channels, 4*model_level_3_channels, model_level_3_channels, 3, 1))\n",
        "        self.enc_3.append(InvResBlock(model_level_3_channels, 4*model_level_3_channels, model_level_4_channels, 3, stride3))\n",
        "\n",
        "        # encoder level 4-Body\n",
        "        self.enc_4 = nn.ModuleList()\n",
        "        for n in range(model_level_4_blocks - 1):\n",
        "            self.enc_4.append(InvResBlock(model_level_4_channels, 4*model_level_4_channels, model_level_4_channels, 3, 1))\n",
        "        self.enc_4.append(InvResBlock(model_level_4_channels, 4*model_level_4_channels, model_level_5_channels, 3, stride4))\n",
        "\n",
        "        # encoder level 5-Body\n",
        "        self.enc_5 = nn.ModuleList()\n",
        "        for n in range(model_level_5_blocks - 1):\n",
        "            self.enc_5.append(InvResBlock(model_level_5_channels, 4*model_level_5_channels, model_level_5_channels, 3, 1))\n",
        "        self.enc_5.append(InvResBlock(model_level_5_channels, 4*model_level_5_channels, model_head_conv_in_channels, 3, stride5))\n",
        "\n",
        "        # decoder-Head\n",
        "        self.dec = nn.ModuleList()\n",
        "        self.dec.append(nn.Conv2d(model_head_conv_in_channels,model_head_conv_out_channels, kernel_size = 1, stride=1, padding=0, dilation=1, groups=1, bias=False, padding_mode='zeros'))\n",
        "        self.dec.append(nn.BatchNorm2d(model_head_conv_out_channels, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True))\n",
        "        self.dec.append(nn.ReLU())\n",
        "        self.dec.append(nn.AdaptiveAvgPool2d((1, 1)))\n",
        "        self.dec.append(nn.Flatten())\n",
        "        self.dec.append(nn.Linear(model_head_conv_out_channels, data_num_classes, bias=True))\n",
        "\n",
        "    # forward path\n",
        "    def forward(self, x):\n",
        "\n",
        "        # map input x to output y for the network defined in table 1 of the\n",
        "        # paper via connecting the operators defined in the initialization\n",
        "        # and return output y\n",
        "\n",
        "        for layer in self.tail:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 1\n",
        "        for layer in self.enc_1:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 2\n",
        "        for layer in self.enc_2:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 3\n",
        "        for layer in self.enc_3:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 4\n",
        "        for layer in self.enc_4:\n",
        "            x = layer(x)\n",
        "\n",
        "        # encoder level 5\n",
        "        for layer in self.enc_5:\n",
        "            x = layer(x)\n",
        "\n",
        "        # decoder\n",
        "        for layer in self.dec:\n",
        "            x = layer(x)\n",
        "\n",
        "\n",
        "        # return\n",
        "        return x\n",
        "\n",
        "# create\n",
        "# add necessary parameters to the init function to create the model defined\n",
        "# in table 1 of the paper\n",
        "model = Model(DATA_NUM_CHANNELS,\n",
        "              MODEL_LEVEL_1_BLOCKS, MODEL_LEVEL_1_CHANNELS,\n",
        "              MODEL_LEVEL_2_BLOCKS, MODEL_LEVEL_2_CHANNELS,\n",
        "              MODEL_LEVEL_3_BLOCKS, MODEL_LEVEL_3_CHANNELS,\n",
        "              MODEL_LEVEL_4_BLOCKS, MODEL_LEVEL_4_CHANNELS,\n",
        "              MODEL_LEVEL_5_BLOCKS, MODEL_LEVEL_5_CHANNELS,\n",
        "              MODEL_HEAD_CONV_IN_CHANNELS, MODEL_HEAD_CONV_OUT_CHANNELS, DATA_NUM_CLASSES) \n",
        "\n",
        "# enable data parallelization for multi GPU systems\n",
        "if (torch.cuda.device_count() > 1):\n",
        "    model = nn.DataParallel(model)\n",
        "print('Using {0:d} GPU(s)'.format(torch.cuda.device_count()), flush=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1 GPU(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vePm7dKwdi5A"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# ERROR AND OPTIMIZER                                                           #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# error (softmax cross entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# learning rate schedule\n",
        "def lr_schedule(epoch):\n",
        "\n",
        "    # linear warmup followed by 1/2 wave cosine decay\n",
        "    if epoch < TRAIN_LR_INIT_EPOCHS:\n",
        "        lr = (TRAIN_LR_MAX - TRAIN_LR_INIT)*(float(epoch)/TRAIN_LR_INIT_EPOCHS) + TRAIN_LR_INIT\n",
        "    else:\n",
        "        lr = TRAIN_LR_FINAL + 0.5*(TRAIN_LR_MAX - TRAIN_LR_FINAL)*(1.0 + math.cos(((float(epoch) - TRAIN_LR_INIT_EPOCHS)/(TRAIN_LR_FINAL_EPOCHS - 1.0))*math.pi))\n",
        "\n",
        "    return lr\n",
        "\n",
        "# optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, dampening=0.0, weight_decay=5e-5, nesterov=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LWCL0HhdnWr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef1737e2-a129-4a1c-caad-8320ace3e29c"
      },
      "source": [
        "#################################################################################\n",
        "#                                                                               #\n",
        "# TRAINING                                                                      #\n",
        "#                                                                               #\n",
        "#################################################################################\n",
        "\n",
        "# start epoch\n",
        "start_epoch = 0\n",
        "\n",
        "# specify the device as the GPU if present with fallback to the CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# transfer the network to the device\n",
        "model.to(device)\n",
        "\n",
        "# load the last checkpoint\n",
        "if (FILE_LOAD == True):\n",
        "    checkpoint = torch.load(FILE_NAME_CHECK)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if (FILE_NEW_OPTIMIZER == False):\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if (FILE_EXTEND_TRAINING == False):\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "# initialize the epoch\n",
        "accuracy_best      = 0\n",
        "start_time_display = time.time()\n",
        "start_time_epoch   = time.time()\n",
        "\n",
        "# cycle through the epochs\n",
        "for epoch in range(start_epoch, TRAIN_NUM_EPOCHS):\n",
        "\n",
        "    # initialize epoch training\n",
        "    model.train()\n",
        "    training_loss = 0.0\n",
        "    num_batches   = 0\n",
        "    num_display   = 0\n",
        "\n",
        "    # set the learning rate for the epoch\n",
        "    for g in optimizer.param_groups:\n",
        "        g['lr'] = lr_schedule(epoch)\n",
        "\n",
        "    # cycle through the training data set\n",
        "    for data in dataloader_train:\n",
        "\n",
        "        # extract a batch of data and move it to the appropriate device\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass, loss, backward pass and weight update\n",
        "        outputs = model(inputs)\n",
        "        loss    = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update statistics\n",
        "        training_loss = training_loss + loss.item()\n",
        "        num_batches   = num_batches + 1\n",
        "        num_display   = num_display + DATA_BATCH_SIZE\n",
        "\n",
        "        # display intra epoch results\n",
        "        if (num_display > TRAIN_INTRA_EPOCH_DISPLAY):\n",
        "            num_display          = 0\n",
        "            elapsed_time_display = time.time() - start_time_display\n",
        "            start_time_display   = time.time()\n",
        "            print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f}'.format(epoch, elapsed_time_display, lr_schedule(epoch), (training_loss / num_batches) / DATA_BATCH_SIZE), flush=True)\n",
        "\n",
        "    # initialize epoch testing\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total   = 0\n",
        "\n",
        "    # no weight update / no gradient needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # cycle through the testing data set\n",
        "        for data in dataloader_test:\n",
        "\n",
        "            # extract a batch of data and move it to the appropriate device\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # forward pass and prediction\n",
        "            outputs      = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # update test set statistics\n",
        "            test_total   = test_total + labels.size(0)\n",
        "            test_correct = test_correct + (predicted == labels).sum().item()\n",
        "\n",
        "    # epoch statistics\n",
        "    elapsed_time_epoch = time.time() - start_time_epoch\n",
        "    start_time_epoch   = time.time()\n",
        "    print('Epoch {0:3d} Time {1:8.1f} lr = {2:8.6f} avg loss = {3:8.6f} accuracy = {4:5.2f}'.format(epoch, elapsed_time_epoch, lr_schedule(epoch), (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total)), flush=True)\n",
        "\n",
        "    # save a checkpoint\n",
        "    if (FILE_SAVE == True):\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_CHECK)\n",
        "\n",
        "    # save the best model\n",
        "    accuracy_epoch = 100.0 * test_correct / test_total\n",
        "    if ((FILE_SAVE == True) and (accuracy_epoch >= accuracy_best)):\n",
        "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}, FILE_NAME_BEST)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch   0 Time     27.0 lr = 0.002000 avg loss = 0.017993\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.017911\n",
            "Epoch   0 Time     26.6 lr = 0.002000 avg loss = 0.017819\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.017724\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.017617\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.017504\n",
            "Epoch   0 Time     26.6 lr = 0.002000 avg loss = 0.017384\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.017262\n",
            "Epoch   0 Time     26.6 lr = 0.002000 avg loss = 0.017141\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.017018\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.016897\n",
            "Epoch   0 Time     26.7 lr = 0.002000 avg loss = 0.016779\n",
            "Epoch   0 Time    342.0 lr = 0.002000 avg loss = 0.016711 accuracy = 10.38\n",
            "Epoch   1 Time     48.4 lr = 0.041600 avg loss = 0.015374\n",
            "Epoch   1 Time     26.4 lr = 0.041600 avg loss = 0.014985\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.014695\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.014444\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.014235\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.014026\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.013840\n",
            "Epoch   1 Time     26.4 lr = 0.041600 avg loss = 0.013661\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.013499\n",
            "Epoch   1 Time     26.4 lr = 0.041600 avg loss = 0.013343\n",
            "Epoch   1 Time     26.4 lr = 0.041600 avg loss = 0.013202\n",
            "Epoch   1 Time     26.5 lr = 0.041600 avg loss = 0.013070\n",
            "Epoch   1 Time    339.3 lr = 0.041600 avg loss = 0.012989 accuracy = 29.54\n",
            "Epoch   2 Time     48.2 lr = 0.081200 avg loss = 0.011752\n",
            "Epoch   2 Time     26.5 lr = 0.081200 avg loss = 0.011712\n",
            "Epoch   2 Time     26.5 lr = 0.081200 avg loss = 0.011641\n",
            "Epoch   2 Time     26.4 lr = 0.081200 avg loss = 0.011558\n",
            "Epoch   2 Time     26.4 lr = 0.081200 avg loss = 0.011460\n",
            "Epoch   2 Time     26.4 lr = 0.081200 avg loss = 0.011378\n",
            "Epoch   2 Time     26.4 lr = 0.081200 avg loss = 0.011291\n",
            "Epoch   2 Time     26.5 lr = 0.081200 avg loss = 0.011200\n",
            "Epoch   2 Time     26.4 lr = 0.081200 avg loss = 0.011121\n",
            "Epoch   2 Time     26.5 lr = 0.081200 avg loss = 0.011049\n",
            "Epoch   2 Time     26.5 lr = 0.081200 avg loss = 0.010983\n",
            "Epoch   2 Time     26.4 lr = 0.081200 avg loss = 0.010908\n",
            "Epoch   2 Time    339.0 lr = 0.081200 avg loss = 0.010873 accuracy = 35.42\n",
            "Epoch   3 Time     48.2 lr = 0.120800 avg loss = 0.010224\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010304\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010304\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010268\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010234\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010152\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010112\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010066\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.010011\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.009962\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.009900\n",
            "Epoch   3 Time     26.4 lr = 0.120800 avg loss = 0.009849\n",
            "Epoch   3 Time    338.6 lr = 0.120800 avg loss = 0.009820 accuracy = 41.32\n",
            "Epoch   4 Time     48.2 lr = 0.160400 avg loss = 0.009280\n",
            "Epoch   4 Time     26.4 lr = 0.160400 avg loss = 0.009360\n",
            "Epoch   4 Time     26.4 lr = 0.160400 avg loss = 0.009371\n",
            "Epoch   4 Time     26.6 lr = 0.160400 avg loss = 0.009368\n",
            "Epoch   4 Time     26.6 lr = 0.160400 avg loss = 0.009332\n",
            "Epoch   4 Time     26.5 lr = 0.160400 avg loss = 0.009290\n",
            "Epoch   4 Time     26.4 lr = 0.160400 avg loss = 0.009267\n",
            "Epoch   4 Time     26.5 lr = 0.160400 avg loss = 0.009241\n",
            "Epoch   4 Time     26.4 lr = 0.160400 avg loss = 0.009210\n",
            "Epoch   4 Time     26.4 lr = 0.160400 avg loss = 0.009171\n",
            "Epoch   4 Time     26.5 lr = 0.160400 avg loss = 0.009142\n",
            "Epoch   4 Time     26.3 lr = 0.160400 avg loss = 0.009105\n",
            "Epoch   4 Time    339.1 lr = 0.160400 avg loss = 0.009088 accuracy = 44.38\n",
            "Epoch   5 Time     48.0 lr = 0.200000 avg loss = 0.008780\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008719\n",
            "Epoch   5 Time     26.3 lr = 0.200000 avg loss = 0.008749\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008721\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008695\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008667\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008630\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008606\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008595\n",
            "Epoch   5 Time     26.5 lr = 0.200000 avg loss = 0.008576\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008551\n",
            "Epoch   5 Time     26.4 lr = 0.200000 avg loss = 0.008554\n",
            "Epoch   5 Time    338.3 lr = 0.200000 avg loss = 0.008535 accuracy = 46.74\n",
            "Epoch   6 Time     48.1 lr = 0.199795 avg loss = 0.008155\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008207\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008156\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008113\n",
            "Epoch   6 Time     26.5 lr = 0.199795 avg loss = 0.008109\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008078\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008073\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008050\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008038\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008022\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.008010\n",
            "Epoch   6 Time     26.4 lr = 0.199795 avg loss = 0.007997\n",
            "Epoch   6 Time    338.3 lr = 0.199795 avg loss = 0.007995 accuracy = 47.94\n",
            "Epoch   7 Time     48.2 lr = 0.199180 avg loss = 0.007676\n",
            "Epoch   7 Time     26.6 lr = 0.199180 avg loss = 0.007647\n",
            "Epoch   7 Time     26.6 lr = 0.199180 avg loss = 0.007627\n",
            "Epoch   7 Time     26.6 lr = 0.199180 avg loss = 0.007644\n",
            "Epoch   7 Time     26.6 lr = 0.199180 avg loss = 0.007643\n",
            "Epoch   7 Time     26.5 lr = 0.199180 avg loss = 0.007646\n",
            "Epoch   7 Time     26.5 lr = 0.199180 avg loss = 0.007635\n",
            "Epoch   7 Time     26.5 lr = 0.199180 avg loss = 0.007627\n",
            "Epoch   7 Time     26.5 lr = 0.199180 avg loss = 0.007617\n",
            "Epoch   7 Time     26.5 lr = 0.199180 avg loss = 0.007613\n",
            "Epoch   7 Time     26.4 lr = 0.199180 avg loss = 0.007615\n",
            "Epoch   7 Time     26.4 lr = 0.199180 avg loss = 0.007606\n",
            "Epoch   7 Time    339.8 lr = 0.199180 avg loss = 0.007604 accuracy = 49.38\n",
            "Epoch   8 Time     48.1 lr = 0.198158 avg loss = 0.007375\n",
            "Epoch   8 Time     26.5 lr = 0.198158 avg loss = 0.007384\n",
            "Epoch   8 Time     26.5 lr = 0.198158 avg loss = 0.007359\n",
            "Epoch   8 Time     26.4 lr = 0.198158 avg loss = 0.007344\n",
            "Epoch   8 Time     26.6 lr = 0.198158 avg loss = 0.007356\n",
            "Epoch   8 Time     26.6 lr = 0.198158 avg loss = 0.007355\n",
            "Epoch   8 Time     26.6 lr = 0.198158 avg loss = 0.007351\n",
            "Epoch   8 Time     26.6 lr = 0.198158 avg loss = 0.007330\n",
            "Epoch   8 Time     26.5 lr = 0.198158 avg loss = 0.007342\n",
            "Epoch   8 Time     26.5 lr = 0.198158 avg loss = 0.007333\n",
            "Epoch   8 Time     26.4 lr = 0.198158 avg loss = 0.007336\n",
            "Epoch   8 Time     26.5 lr = 0.198158 avg loss = 0.007324\n",
            "Epoch   8 Time    340.0 lr = 0.198158 avg loss = 0.007319 accuracy = 54.20\n",
            "Epoch   9 Time     48.2 lr = 0.196733 avg loss = 0.007175\n",
            "Epoch   9 Time     26.4 lr = 0.196733 avg loss = 0.007101\n",
            "Epoch   9 Time     26.4 lr = 0.196733 avg loss = 0.007119\n",
            "Epoch   9 Time     26.5 lr = 0.196733 avg loss = 0.007101\n",
            "Epoch   9 Time     26.5 lr = 0.196733 avg loss = 0.007080\n",
            "Epoch   9 Time     26.5 lr = 0.196733 avg loss = 0.007092\n",
            "Epoch   9 Time     26.5 lr = 0.196733 avg loss = 0.007099\n",
            "Epoch   9 Time     26.5 lr = 0.196733 avg loss = 0.007091\n",
            "Epoch   9 Time     26.4 lr = 0.196733 avg loss = 0.007084\n",
            "Epoch   9 Time     26.5 lr = 0.196733 avg loss = 0.007089\n",
            "Epoch   9 Time     26.4 lr = 0.196733 avg loss = 0.007093\n",
            "Epoch   9 Time     26.4 lr = 0.196733 avg loss = 0.007086\n",
            "Epoch   9 Time    339.1 lr = 0.196733 avg loss = 0.007079 accuracy = 54.58\n",
            "Epoch  10 Time     48.1 lr = 0.194911 avg loss = 0.006857\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006827\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006802\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006812\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006825\n",
            "Epoch  10 Time     26.5 lr = 0.194911 avg loss = 0.006851\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006848\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006841\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006852\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006854\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006857\n",
            "Epoch  10 Time     26.4 lr = 0.194911 avg loss = 0.006863\n",
            "Epoch  10 Time    338.8 lr = 0.194911 avg loss = 0.006851 accuracy = 57.20\n",
            "Epoch  11 Time     48.2 lr = 0.192699 avg loss = 0.006685\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006702\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006699\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006687\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006684\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006691\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006700\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006706\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006707\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006702\n",
            "Epoch  11 Time     26.3 lr = 0.192699 avg loss = 0.006703\n",
            "Epoch  11 Time     26.4 lr = 0.192699 avg loss = 0.006714\n",
            "Epoch  11 Time    338.3 lr = 0.192699 avg loss = 0.006709 accuracy = 57.54\n",
            "Epoch  12 Time     48.0 lr = 0.190107 avg loss = 0.006472\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006486\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006531\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006499\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006508\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006504\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006526\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006514\n",
            "Epoch  12 Time     26.5 lr = 0.190107 avg loss = 0.006514\n",
            "Epoch  12 Time     26.3 lr = 0.190107 avg loss = 0.006520\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006523\n",
            "Epoch  12 Time     26.4 lr = 0.190107 avg loss = 0.006521\n",
            "Epoch  12 Time    338.3 lr = 0.190107 avg loss = 0.006525 accuracy = 58.52\n",
            "Epoch  13 Time     48.0 lr = 0.187145 avg loss = 0.006295\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006321\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006309\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006356\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006377\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006372\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006375\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006392\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006407\n",
            "Epoch  13 Time     26.4 lr = 0.187145 avg loss = 0.006402\n",
            "Epoch  13 Time     26.5 lr = 0.187145 avg loss = 0.006402\n",
            "Epoch  13 Time     26.5 lr = 0.187145 avg loss = 0.006414\n",
            "Epoch  13 Time    338.7 lr = 0.187145 avg loss = 0.006418 accuracy = 59.00\n",
            "Epoch  14 Time     48.1 lr = 0.183825 avg loss = 0.006315\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006299\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006258\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006254\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006284\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006272\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006283\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006282\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006288\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006292\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006296\n",
            "Epoch  14 Time     26.4 lr = 0.183825 avg loss = 0.006287\n",
            "Epoch  14 Time    338.3 lr = 0.183825 avg loss = 0.006292 accuracy = 57.68\n",
            "Epoch  15 Time     48.0 lr = 0.180161 avg loss = 0.006133\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006141\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006162\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006171\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006178\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006178\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006173\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006182\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006187\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006195\n",
            "Epoch  15 Time     26.4 lr = 0.180161 avg loss = 0.006193\n",
            "Epoch  15 Time     26.3 lr = 0.180161 avg loss = 0.006187\n",
            "Epoch  15 Time    338.2 lr = 0.180161 avg loss = 0.006183 accuracy = 60.62\n",
            "Epoch  16 Time     48.0 lr = 0.176168 avg loss = 0.005812\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.005865\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.005922\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.005943\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.005980\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006011\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006011\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006016\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006021\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006034\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006037\n",
            "Epoch  16 Time     26.4 lr = 0.176168 avg loss = 0.006042\n",
            "Epoch  16 Time    338.4 lr = 0.176168 avg loss = 0.006044 accuracy = 58.66\n",
            "Epoch  17 Time     48.2 lr = 0.171863 avg loss = 0.005957\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.005928\n",
            "Epoch  17 Time     26.5 lr = 0.171863 avg loss = 0.005950\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.005957\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.005981\n",
            "Epoch  17 Time     26.5 lr = 0.171863 avg loss = 0.005982\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.006000\n",
            "Epoch  17 Time     26.5 lr = 0.171863 avg loss = 0.005996\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.005996\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.005999\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.006008\n",
            "Epoch  17 Time     26.4 lr = 0.171863 avg loss = 0.006001\n",
            "Epoch  17 Time    338.8 lr = 0.171863 avg loss = 0.006002 accuracy = 57.46\n",
            "Epoch  18 Time     48.0 lr = 0.167263 avg loss = 0.005812\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005901\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005875\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005879\n",
            "Epoch  18 Time     26.5 lr = 0.167263 avg loss = 0.005883\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005878\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005870\n",
            "Epoch  18 Time     26.5 lr = 0.167263 avg loss = 0.005881\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005892\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005899\n",
            "Epoch  18 Time     26.4 lr = 0.167263 avg loss = 0.005903\n",
            "Epoch  18 Time     26.5 lr = 0.167263 avg loss = 0.005904\n",
            "Epoch  18 Time    338.7 lr = 0.167263 avg loss = 0.005905 accuracy = 61.96\n",
            "Epoch  19 Time     48.3 lr = 0.162387 avg loss = 0.005796\n",
            "Epoch  19 Time     26.5 lr = 0.162387 avg loss = 0.005801\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005763\n",
            "Epoch  19 Time     26.5 lr = 0.162387 avg loss = 0.005770\n",
            "Epoch  19 Time     26.5 lr = 0.162387 avg loss = 0.005797\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005802\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005793\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005786\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005793\n",
            "Epoch  19 Time     26.3 lr = 0.162387 avg loss = 0.005798\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005793\n",
            "Epoch  19 Time     26.4 lr = 0.162387 avg loss = 0.005794\n",
            "Epoch  19 Time    338.8 lr = 0.162387 avg loss = 0.005786 accuracy = 63.76\n",
            "Epoch  20 Time     48.1 lr = 0.157254 avg loss = 0.005610\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005636\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005658\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005687\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005706\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005710\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005729\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005722\n",
            "Epoch  20 Time     26.3 lr = 0.157254 avg loss = 0.005734\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005733\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005732\n",
            "Epoch  20 Time     26.4 lr = 0.157254 avg loss = 0.005743\n",
            "Epoch  20 Time    338.3 lr = 0.157254 avg loss = 0.005743 accuracy = 63.50\n",
            "Epoch  21 Time     48.1 lr = 0.151887 avg loss = 0.005455\n",
            "Epoch  21 Time     26.5 lr = 0.151887 avg loss = 0.005502\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005526\n",
            "Epoch  21 Time     26.3 lr = 0.151887 avg loss = 0.005533\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005557\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005582\n",
            "Epoch  21 Time     26.3 lr = 0.151887 avg loss = 0.005583\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005611\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005608\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005607\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005608\n",
            "Epoch  21 Time     26.4 lr = 0.151887 avg loss = 0.005619\n",
            "Epoch  21 Time    338.5 lr = 0.151887 avg loss = 0.005629 accuracy = 65.34\n",
            "Epoch  22 Time     48.0 lr = 0.146308 avg loss = 0.005339\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005440\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005499\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005522\n",
            "Epoch  22 Time     26.3 lr = 0.146308 avg loss = 0.005547\n",
            "Epoch  22 Time     26.3 lr = 0.146308 avg loss = 0.005525\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005545\n",
            "Epoch  22 Time     26.3 lr = 0.146308 avg loss = 0.005562\n",
            "Epoch  22 Time     26.3 lr = 0.146308 avg loss = 0.005569\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005577\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005591\n",
            "Epoch  22 Time     26.4 lr = 0.146308 avg loss = 0.005599\n",
            "Epoch  22 Time    338.0 lr = 0.146308 avg loss = 0.005604 accuracy = 62.34\n",
            "Epoch  23 Time     48.1 lr = 0.140538 avg loss = 0.005366\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005380\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005398\n",
            "Epoch  23 Time     26.3 lr = 0.140538 avg loss = 0.005442\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005447\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005436\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005453\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005460\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005465\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005471\n",
            "Epoch  23 Time     26.4 lr = 0.140538 avg loss = 0.005479\n",
            "Epoch  23 Time     26.5 lr = 0.140538 avg loss = 0.005483\n",
            "Epoch  23 Time    338.5 lr = 0.140538 avg loss = 0.005490 accuracy = 65.64\n",
            "Epoch  24 Time     48.1 lr = 0.134602 avg loss = 0.005339\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005343\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005340\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005328\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005362\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005381\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005391\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005399\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005405\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005406\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005404\n",
            "Epoch  24 Time     26.4 lr = 0.134602 avg loss = 0.005404\n",
            "Epoch  24 Time    338.5 lr = 0.134602 avg loss = 0.005404 accuracy = 65.64\n",
            "Epoch  25 Time     48.1 lr = 0.128524 avg loss = 0.005230\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005249\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005253\n",
            "Epoch  25 Time     26.5 lr = 0.128524 avg loss = 0.005244\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005274\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005282\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005293\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005305\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005313\n",
            "Epoch  25 Time     26.3 lr = 0.128524 avg loss = 0.005306\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005311\n",
            "Epoch  25 Time     26.4 lr = 0.128524 avg loss = 0.005317\n",
            "Epoch  25 Time    338.4 lr = 0.128524 avg loss = 0.005331 accuracy = 65.20\n",
            "Epoch  26 Time     48.1 lr = 0.122330 avg loss = 0.005153\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005142\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005191\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005209\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005223\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005245\n",
            "Epoch  26 Time     26.3 lr = 0.122330 avg loss = 0.005255\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005263\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005269\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005265\n",
            "Epoch  26 Time     26.4 lr = 0.122330 avg loss = 0.005269\n",
            "Epoch  26 Time     26.3 lr = 0.122330 avg loss = 0.005267\n",
            "Epoch  26 Time    338.2 lr = 0.122330 avg loss = 0.005273 accuracy = 65.42\n",
            "Epoch  27 Time     48.2 lr = 0.116044 avg loss = 0.005121\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005171\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005135\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005148\n",
            "Epoch  27 Time     26.3 lr = 0.116044 avg loss = 0.005166\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005156\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005166\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005165\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005167\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005171\n",
            "Epoch  27 Time     26.4 lr = 0.116044 avg loss = 0.005178\n",
            "Epoch  27 Time     26.5 lr = 0.116044 avg loss = 0.005184\n",
            "Epoch  27 Time    338.7 lr = 0.116044 avg loss = 0.005185 accuracy = 67.48\n",
            "Epoch  28 Time     48.2 lr = 0.109693 avg loss = 0.005009\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.004965\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.004968\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.004999\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.005004\n",
            "Epoch  28 Time     26.5 lr = 0.109693 avg loss = 0.005027\n",
            "Epoch  28 Time     26.5 lr = 0.109693 avg loss = 0.005044\n",
            "Epoch  28 Time     26.5 lr = 0.109693 avg loss = 0.005073\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.005085\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.005080\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.005080\n",
            "Epoch  28 Time     26.4 lr = 0.109693 avg loss = 0.005087\n",
            "Epoch  28 Time    339.0 lr = 0.109693 avg loss = 0.005093 accuracy = 65.72\n",
            "Epoch  29 Time     48.0 lr = 0.103302 avg loss = 0.004866\n",
            "Epoch  29 Time     26.4 lr = 0.103302 avg loss = 0.004882\n",
            "Epoch  29 Time     26.5 lr = 0.103302 avg loss = 0.004899\n",
            "Epoch  29 Time     26.4 lr = 0.103302 avg loss = 0.004940\n",
            "Epoch  29 Time     26.4 lr = 0.103302 avg loss = 0.004939\n",
            "Epoch  29 Time     26.5 lr = 0.103302 avg loss = 0.004965\n",
            "Epoch  29 Time     26.5 lr = 0.103302 avg loss = 0.004985\n",
            "Epoch  29 Time     26.4 lr = 0.103302 avg loss = 0.004992\n",
            "Epoch  29 Time     26.5 lr = 0.103302 avg loss = 0.004996\n",
            "Epoch  29 Time     26.5 lr = 0.103302 avg loss = 0.005005\n",
            "Epoch  29 Time     26.4 lr = 0.103302 avg loss = 0.005012\n",
            "Epoch  29 Time     26.4 lr = 0.103302 avg loss = 0.005024\n",
            "Epoch  29 Time    339.0 lr = 0.103302 avg loss = 0.005028 accuracy = 63.58\n",
            "Epoch  30 Time     48.1 lr = 0.096898 avg loss = 0.004825\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004829\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004830\n",
            "Epoch  30 Time     26.5 lr = 0.096898 avg loss = 0.004843\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004876\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004891\n",
            "Epoch  30 Time     26.5 lr = 0.096898 avg loss = 0.004903\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004925\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004939\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004943\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004949\n",
            "Epoch  30 Time     26.4 lr = 0.096898 avg loss = 0.004959\n",
            "Epoch  30 Time    338.7 lr = 0.096898 avg loss = 0.004962 accuracy = 65.06\n",
            "Epoch  31 Time     48.1 lr = 0.090507 avg loss = 0.004716\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004769\n",
            "Epoch  31 Time     26.5 lr = 0.090507 avg loss = 0.004745\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004752\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004761\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004779\n",
            "Epoch  31 Time     26.3 lr = 0.090507 avg loss = 0.004791\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004793\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004801\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004814\n",
            "Epoch  31 Time     26.4 lr = 0.090507 avg loss = 0.004829\n",
            "Epoch  31 Time     26.3 lr = 0.090507 avg loss = 0.004836\n",
            "Epoch  31 Time    338.4 lr = 0.090507 avg loss = 0.004843 accuracy = 65.68\n",
            "Epoch  32 Time     48.0 lr = 0.084156 avg loss = 0.004677\n",
            "Epoch  32 Time     26.4 lr = 0.084156 avg loss = 0.004726\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004747\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004752\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004733\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004729\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004746\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004747\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004760\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004773\n",
            "Epoch  32 Time     26.3 lr = 0.084156 avg loss = 0.004776\n",
            "Epoch  32 Time     26.4 lr = 0.084156 avg loss = 0.004778\n",
            "Epoch  32 Time    337.6 lr = 0.084156 avg loss = 0.004794 accuracy = 67.80\n",
            "Epoch  33 Time     48.0 lr = 0.077870 avg loss = 0.004625\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004617\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004658\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004648\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004665\n",
            "Epoch  33 Time     26.4 lr = 0.077870 avg loss = 0.004682\n",
            "Epoch  33 Time     26.4 lr = 0.077870 avg loss = 0.004683\n",
            "Epoch  33 Time     26.4 lr = 0.077870 avg loss = 0.004690\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004682\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004687\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004697\n",
            "Epoch  33 Time     26.3 lr = 0.077870 avg loss = 0.004691\n",
            "Epoch  33 Time    337.6 lr = 0.077870 avg loss = 0.004692 accuracy = 67.72\n",
            "Epoch  34 Time     48.0 lr = 0.071676 avg loss = 0.004599\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004602\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004599\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004566\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004551\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004561\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004568\n",
            "Epoch  34 Time     26.4 lr = 0.071676 avg loss = 0.004586\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004596\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004597\n",
            "Epoch  34 Time     26.3 lr = 0.071676 avg loss = 0.004601\n",
            "Epoch  34 Time     26.4 lr = 0.071676 avg loss = 0.004598\n",
            "Epoch  34 Time    337.4 lr = 0.071676 avg loss = 0.004609 accuracy = 67.96\n",
            "Epoch  35 Time     48.0 lr = 0.065598 avg loss = 0.004516\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004460\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004486\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004473\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004490\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004484\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004497\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004492\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004499\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004504\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004516\n",
            "Epoch  35 Time     26.3 lr = 0.065598 avg loss = 0.004521\n",
            "Epoch  35 Time    337.4 lr = 0.065598 avg loss = 0.004525 accuracy = 67.64\n",
            "Epoch  36 Time     47.9 lr = 0.059662 avg loss = 0.004346\n",
            "Epoch  36 Time     26.3 lr = 0.059662 avg loss = 0.004375\n",
            "Epoch  36 Time     26.3 lr = 0.059662 avg loss = 0.004401\n",
            "Epoch  36 Time     26.3 lr = 0.059662 avg loss = 0.004412\n",
            "Epoch  36 Time     26.3 lr = 0.059662 avg loss = 0.004386\n",
            "Epoch  36 Time     26.4 lr = 0.059662 avg loss = 0.004416\n",
            "Epoch  36 Time     26.6 lr = 0.059662 avg loss = 0.004418\n",
            "Epoch  36 Time     26.6 lr = 0.059662 avg loss = 0.004428\n",
            "Epoch  36 Time     26.6 lr = 0.059662 avg loss = 0.004440\n",
            "Epoch  36 Time     26.6 lr = 0.059662 avg loss = 0.004436\n",
            "Epoch  36 Time     26.4 lr = 0.059662 avg loss = 0.004431\n",
            "Epoch  36 Time     26.3 lr = 0.059662 avg loss = 0.004437\n",
            "Epoch  36 Time    338.7 lr = 0.059662 avg loss = 0.004437 accuracy = 67.90\n",
            "Epoch  37 Time     48.0 lr = 0.053892 avg loss = 0.004234\n",
            "Epoch  37 Time     26.4 lr = 0.053892 avg loss = 0.004220\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004263\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004293\n",
            "Epoch  37 Time     26.4 lr = 0.053892 avg loss = 0.004309\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004319\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004312\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004312\n",
            "Epoch  37 Time     26.4 lr = 0.053892 avg loss = 0.004323\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004334\n",
            "Epoch  37 Time     26.4 lr = 0.053892 avg loss = 0.004348\n",
            "Epoch  37 Time     26.3 lr = 0.053892 avg loss = 0.004351\n",
            "Epoch  37 Time    337.6 lr = 0.053892 avg loss = 0.004353 accuracy = 70.16\n",
            "Epoch  38 Time     47.9 lr = 0.048313 avg loss = 0.004259\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004248\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004237\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004257\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004243\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004248\n",
            "Epoch  38 Time     26.4 lr = 0.048313 avg loss = 0.004258\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004272\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004272\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004277\n",
            "Epoch  38 Time     26.4 lr = 0.048313 avg loss = 0.004276\n",
            "Epoch  38 Time     26.3 lr = 0.048313 avg loss = 0.004278\n",
            "Epoch  38 Time    337.3 lr = 0.048313 avg loss = 0.004280 accuracy = 69.38\n",
            "Epoch  39 Time     48.0 lr = 0.042946 avg loss = 0.004116\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004113\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004101\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004114\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004132\n",
            "Epoch  39 Time     26.4 lr = 0.042946 avg loss = 0.004157\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004166\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004162\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004168\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004174\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004178\n",
            "Epoch  39 Time     26.3 lr = 0.042946 avg loss = 0.004175\n",
            "Epoch  39 Time    337.2 lr = 0.042946 avg loss = 0.004178 accuracy = 69.92\n",
            "Epoch  40 Time     47.9 lr = 0.037813 avg loss = 0.004109\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004031\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004056\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004051\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004055\n",
            "Epoch  40 Time     26.4 lr = 0.037813 avg loss = 0.004067\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004069\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004073\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004090\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004095\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004092\n",
            "Epoch  40 Time     26.3 lr = 0.037813 avg loss = 0.004088\n",
            "Epoch  40 Time    337.3 lr = 0.037813 avg loss = 0.004097 accuracy = 69.54\n",
            "Epoch  41 Time     47.9 lr = 0.032937 avg loss = 0.003949\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.004009\n",
            "Epoch  41 Time     26.4 lr = 0.032937 avg loss = 0.003986\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.003968\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.003976\n",
            "Epoch  41 Time     26.4 lr = 0.032937 avg loss = 0.003977\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.003990\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.003990\n",
            "Epoch  41 Time     26.4 lr = 0.032937 avg loss = 0.003989\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.003998\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.004002\n",
            "Epoch  41 Time     26.3 lr = 0.032937 avg loss = 0.004003\n",
            "Epoch  41 Time    337.3 lr = 0.032937 avg loss = 0.004007 accuracy = 70.76\n",
            "Epoch  42 Time     47.9 lr = 0.028337 avg loss = 0.003957\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003909\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003927\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003928\n",
            "Epoch  42 Time     26.4 lr = 0.028337 avg loss = 0.003954\n",
            "Epoch  42 Time     26.4 lr = 0.028337 avg loss = 0.003930\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003933\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003928\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003926\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003931\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003932\n",
            "Epoch  42 Time     26.3 lr = 0.028337 avg loss = 0.003930\n",
            "Epoch  42 Time    337.4 lr = 0.028337 avg loss = 0.003928 accuracy = 70.32\n",
            "Epoch  43 Time     47.9 lr = 0.024032 avg loss = 0.003793\n",
            "Epoch  43 Time     26.3 lr = 0.024032 avg loss = 0.003802\n",
            "Epoch  43 Time     26.3 lr = 0.024032 avg loss = 0.003855\n",
            "Epoch  43 Time     26.4 lr = 0.024032 avg loss = 0.003849\n",
            "Epoch  43 Time     26.4 lr = 0.024032 avg loss = 0.003856\n",
            "Epoch  43 Time     26.3 lr = 0.024032 avg loss = 0.003853\n",
            "Epoch  43 Time     26.3 lr = 0.024032 avg loss = 0.003840\n",
            "Epoch  43 Time     26.4 lr = 0.024032 avg loss = 0.003842\n",
            "Epoch  43 Time     26.4 lr = 0.024032 avg loss = 0.003861\n",
            "Epoch  43 Time     26.3 lr = 0.024032 avg loss = 0.003858\n",
            "Epoch  43 Time     26.4 lr = 0.024032 avg loss = 0.003857\n",
            "Epoch  43 Time     26.4 lr = 0.024032 avg loss = 0.003851\n",
            "Epoch  43 Time    337.7 lr = 0.024032 avg loss = 0.003853 accuracy = 70.56\n",
            "Epoch  44 Time     47.9 lr = 0.020039 avg loss = 0.003694\n",
            "Epoch  44 Time     26.4 lr = 0.020039 avg loss = 0.003608\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003671\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003671\n",
            "Epoch  44 Time     26.4 lr = 0.020039 avg loss = 0.003694\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003694\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003698\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003711\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003715\n",
            "Epoch  44 Time     26.4 lr = 0.020039 avg loss = 0.003732\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003738\n",
            "Epoch  44 Time     26.3 lr = 0.020039 avg loss = 0.003755\n",
            "Epoch  44 Time    337.6 lr = 0.020039 avg loss = 0.003755 accuracy = 70.44\n",
            "Epoch  45 Time     47.9 lr = 0.016375 avg loss = 0.003588\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003649\n",
            "Epoch  45 Time     26.4 lr = 0.016375 avg loss = 0.003667\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003677\n",
            "Epoch  45 Time     26.4 lr = 0.016375 avg loss = 0.003686\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003674\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003683\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003695\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003689\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003688\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003698\n",
            "Epoch  45 Time     26.3 lr = 0.016375 avg loss = 0.003698\n",
            "Epoch  45 Time    337.5 lr = 0.016375 avg loss = 0.003695 accuracy = 71.72\n",
            "Epoch  46 Time     47.9 lr = 0.013055 avg loss = 0.003548\n",
            "Epoch  46 Time     26.3 lr = 0.013055 avg loss = 0.003599\n",
            "Epoch  46 Time     26.3 lr = 0.013055 avg loss = 0.003598\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003601\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003583\n",
            "Epoch  46 Time     26.3 lr = 0.013055 avg loss = 0.003571\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003572\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003581\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003584\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003596\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003598\n",
            "Epoch  46 Time     26.4 lr = 0.013055 avg loss = 0.003602\n",
            "Epoch  46 Time    338.0 lr = 0.013055 avg loss = 0.003603 accuracy = 71.56\n",
            "Epoch  47 Time     48.0 lr = 0.010093 avg loss = 0.003488\n",
            "Epoch  47 Time     26.4 lr = 0.010093 avg loss = 0.003533\n",
            "Epoch  47 Time     26.4 lr = 0.010093 avg loss = 0.003572\n",
            "Epoch  47 Time     26.4 lr = 0.010093 avg loss = 0.003548\n",
            "Epoch  47 Time     26.4 lr = 0.010093 avg loss = 0.003566\n",
            "Epoch  47 Time     26.5 lr = 0.010093 avg loss = 0.003560\n",
            "Epoch  47 Time     26.7 lr = 0.010093 avg loss = 0.003546\n",
            "Epoch  47 Time     26.5 lr = 0.010093 avg loss = 0.003559\n",
            "Epoch  47 Time     26.4 lr = 0.010093 avg loss = 0.003556\n",
            "Epoch  47 Time     26.5 lr = 0.010093 avg loss = 0.003557\n",
            "Epoch  47 Time     26.5 lr = 0.010093 avg loss = 0.003550\n",
            "Epoch  47 Time     26.4 lr = 0.010093 avg loss = 0.003545\n",
            "Epoch  47 Time    339.2 lr = 0.010093 avg loss = 0.003547 accuracy = 71.06\n",
            "Epoch  48 Time     48.0 lr = 0.007501 avg loss = 0.003465\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003509\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003498\n",
            "Epoch  48 Time     26.5 lr = 0.007501 avg loss = 0.003484\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003490\n",
            "Epoch  48 Time     26.5 lr = 0.007501 avg loss = 0.003489\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003496\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003502\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003496\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003493\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003504\n",
            "Epoch  48 Time     26.4 lr = 0.007501 avg loss = 0.003500\n",
            "Epoch  48 Time    338.7 lr = 0.007501 avg loss = 0.003501 accuracy = 72.06\n",
            "Epoch  49 Time     48.1 lr = 0.005289 avg loss = 0.003411\n",
            "Epoch  49 Time     26.5 lr = 0.005289 avg loss = 0.003402\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003429\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003446\n",
            "Epoch  49 Time     26.3 lr = 0.005289 avg loss = 0.003442\n",
            "Epoch  49 Time     26.3 lr = 0.005289 avg loss = 0.003447\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003430\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003437\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003441\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003442\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003443\n",
            "Epoch  49 Time     26.4 lr = 0.005289 avg loss = 0.003449\n",
            "Epoch  49 Time    338.4 lr = 0.005289 avg loss = 0.003451 accuracy = 71.36\n",
            "Epoch  50 Time     48.0 lr = 0.003467 avg loss = 0.003417\n",
            "Epoch  50 Time     26.4 lr = 0.003467 avg loss = 0.003386\n",
            "Epoch  50 Time     26.4 lr = 0.003467 avg loss = 0.003399\n",
            "Epoch  50 Time     26.7 lr = 0.003467 avg loss = 0.003407\n",
            "Epoch  50 Time     26.7 lr = 0.003467 avg loss = 0.003417\n",
            "Epoch  50 Time     26.7 lr = 0.003467 avg loss = 0.003421\n",
            "Epoch  50 Time     26.6 lr = 0.003467 avg loss = 0.003427\n",
            "Epoch  50 Time     26.5 lr = 0.003467 avg loss = 0.003429\n",
            "Epoch  50 Time     26.4 lr = 0.003467 avg loss = 0.003439\n",
            "Epoch  50 Time     26.5 lr = 0.003467 avg loss = 0.003437\n",
            "Epoch  50 Time     26.4 lr = 0.003467 avg loss = 0.003439\n",
            "Epoch  50 Time     26.4 lr = 0.003467 avg loss = 0.003437\n",
            "Epoch  50 Time    339.8 lr = 0.003467 avg loss = 0.003432 accuracy = 71.88\n",
            "Epoch  51 Time     48.1 lr = 0.002042 avg loss = 0.003424\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003356\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003403\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003400\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003404\n",
            "Epoch  51 Time     26.5 lr = 0.002042 avg loss = 0.003396\n",
            "Epoch  51 Time     26.5 lr = 0.002042 avg loss = 0.003405\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003397\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003394\n",
            "Epoch  51 Time     26.4 lr = 0.002042 avg loss = 0.003400\n",
            "Epoch  51 Time     26.5 lr = 0.002042 avg loss = 0.003401\n",
            "Epoch  51 Time     26.5 lr = 0.002042 avg loss = 0.003395\n",
            "Epoch  51 Time    339.0 lr = 0.002042 avg loss = 0.003396 accuracy = 71.74\n",
            "Epoch  52 Time     48.1 lr = 0.001020 avg loss = 0.003425\n",
            "Epoch  52 Time     26.4 lr = 0.001020 avg loss = 0.003432\n",
            "Epoch  52 Time     26.5 lr = 0.001020 avg loss = 0.003403\n",
            "Epoch  52 Time     26.4 lr = 0.001020 avg loss = 0.003394\n",
            "Epoch  52 Time     26.5 lr = 0.001020 avg loss = 0.003372\n",
            "Epoch  52 Time     26.5 lr = 0.001020 avg loss = 0.003373\n",
            "Epoch  52 Time     26.4 lr = 0.001020 avg loss = 0.003372\n",
            "Epoch  52 Time     26.4 lr = 0.001020 avg loss = 0.003364\n",
            "Epoch  52 Time     26.4 lr = 0.001020 avg loss = 0.003366\n",
            "Epoch  52 Time     26.5 lr = 0.001020 avg loss = 0.003371\n",
            "Epoch  52 Time     26.4 lr = 0.001020 avg loss = 0.003371\n",
            "Epoch  52 Time     26.5 lr = 0.001020 avg loss = 0.003373\n",
            "Epoch  52 Time    338.9 lr = 0.001020 avg loss = 0.003373 accuracy = 71.90\n",
            "Epoch  53 Time     48.1 lr = 0.000405 avg loss = 0.003417\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003389\n",
            "Epoch  53 Time     26.5 lr = 0.000405 avg loss = 0.003391\n",
            "Epoch  53 Time     26.5 lr = 0.000405 avg loss = 0.003375\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003349\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003363\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003354\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003354\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003359\n",
            "Epoch  53 Time     26.3 lr = 0.000405 avg loss = 0.003363\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003362\n",
            "Epoch  53 Time     26.4 lr = 0.000405 avg loss = 0.003364\n",
            "Epoch  53 Time    338.6 lr = 0.000405 avg loss = 0.003371 accuracy = 71.94\n",
            "Epoch  54 Time     48.2 lr = 0.000200 avg loss = 0.003334\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003370\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003383\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003381\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003370\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003365\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003364\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003370\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003368\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003367\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003359\n",
            "Epoch  54 Time     26.4 lr = 0.000200 avg loss = 0.003365\n",
            "Epoch  54 Time    338.6 lr = 0.000200 avg loss = 0.003368 accuracy = 72.08\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}